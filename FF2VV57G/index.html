<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/002_%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/08_SVM/">
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Index - 算法工程师笔记</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../css/extra.css" rel="stylesheet">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#svm" class="nav-link">SVM</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#_1" class="nav-link">一、最优化问题的导出</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#11" class="nav-link">1.1 对问题做一些直观上的简单描述</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#12" class="nav-link">1.2 定义一些概念</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#13-svm" class="nav-link">1.3 SVM推导</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#lagrange-multiplier" class="nav-link">二、拉格朗日乘子法（Lagrange Multiplier）</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#_2" class="nav-link">三、硬间隔和软间隔问题的求解</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#31" class="nav-link">3.1 硬间隔</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#32" class="nav-link">3.2 软间隔</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#smo" class="nav-link">四、SMO算法步骤总结</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#41" class="nav-link">4.1 首先是待优化问题</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#42" class="nav-link">4.2 算法流程中三个地方详细说明</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#43" class="nav-link">4.3 总结</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p align="right">[<a href="/FF2VV57G_no_toc/">隐藏左侧目录栏</a>][<a href="/FF2VV57G/">显示左侧目录栏</a>]</p>

<h1 id="svm">SVM<a class="headerlink" href="#svm" title="Permanent link">#</a></h1>
<p>先贴一下cs229课程的官网地址：http://cs229.stanford.edu/</p>
<h2 id="_1">一、最优化问题的导出<a class="headerlink" href="#_1" title="Permanent link">#</a></h2>
<p>刚开始学习SVM的时候对于该算法中什么是模型，什么是损失函数一直搞不清楚。所以记录一下由直观的现实问题转化到严谨的数学公式的过程。即本文的第一部分的最终目的是得到如下约束问题：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\min_{w,b}\frac{1}{2}||w||^2</div>
<script type="math/tex; mode=display">\min_{w,b}\frac{1}{2}||w||^2</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">st. y^{(i)}(w^Tx^{(i)} + b) \ge 1</div>
<script type="math/tex; mode=display">st. y^{(i)}(w^Tx^{(i)} + b) \ge 1</script>
</div>
<h3 id="11">1.1 对问题做一些直观上的简单描述<a class="headerlink" href="#11" title="Permanent link">#</a></h3>
<p>先从最简单的开始，我们要设计一个解决二分类问题的模型，该模型功能并不强大，它只能解决严格线性可分的问题。对于正例模型输出+1，对于负例模型输出-1。我们的模型中有一个分割超平面：<span class="arithmatex"><span class="MathJax_Preview">w^Tx+b=0</span><script type="math/tex">w^Tx+b=0</script></span>，该分割超平面肯定能将数据分类正确，即<span class="arithmatex"><span class="MathJax_Preview">sign(y) = sign(w^Tx+b)</span><script type="math/tex">sign(y) = sign(w^Tx+b)</script></span>（<span class="arithmatex"><span class="MathJax_Preview">sign(\cdot)</span><script type="math/tex">sign(\cdot)</script></span>是取符号的意思）。</p>
<p>训练集：<span class="arithmatex"><span class="MathJax_Preview">S=\{(x^{(i)}, y^{(i)})\}</span><script type="math/tex">S=\{(x^{(i)}, y^{(i)})\}</script></span>，<span class="arithmatex"><span class="MathJax_Preview">y^{(i)} \in \{-1,+1\}</span><script type="math/tex">y^{(i)} \in \{-1,+1\}</script></span>，<span class="arithmatex"><span class="MathJax_Preview">i=1,...,m</span><script type="math/tex">i=1,...,m</script></span>，其中<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>为样本条数</p>
<p>模型：<span class="arithmatex"><span class="MathJax_Preview">h_{w,b}(x) = g(w^Tx+b)</span><script type="math/tex">h_{w,b}(x) = g(w^Tx+b)</script></span>其中 <span class="arithmatex"><span class="MathJax_Preview">g(z)= \begin{cases} +1, &amp; z \ge 0 \\ -1, &amp; z &lt; 0  \end{cases}</span><script type="math/tex">g(z)= \begin{cases} +1, & z \ge 0 \\ -1, & z < 0  \end{cases}</script></span></p>
<h3 id="12">1.2 定义一些概念<a class="headerlink" href="#12" title="Permanent link">#</a></h3>
<p>点到直线的距离推广到高维空间：<span class="arithmatex"><span class="MathJax_Preview">d = \frac{|w^T\vec X+b|}{||w||}</span><script type="math/tex">d = \frac{|w^T\vec X+b|}{||w||}</script></span></p>
<p>定义单个样本点到分割超平面的几何距离为<span class="arithmatex"><span class="MathJax_Preview">\gamma^{(i)}</span><script type="math/tex">\gamma^{(i)}</script></span>，由上面的公式可知：<span class="arithmatex"><span class="MathJax_Preview">\gamma^{(i)} = \frac{|w^Tx^{(i)}+b|}{||w||}</span><script type="math/tex">\gamma^{(i)} = \frac{|w^Tx^{(i)}+b|}{||w||}</script></span></p>
<p>由于训练集中<span class="arithmatex"><span class="MathJax_Preview">y^{(i)} \in \{-1,+1\}</span><script type="math/tex">y^{(i)} \in \{-1,+1\}</script></span>并且<span class="arithmatex"><span class="MathJax_Preview">sign(y^{(i)}) = sign(w^Tx^{(i)} + b)</span><script type="math/tex">sign(y^{(i)}) = sign(w^Tx^{(i)} + b)</script></span>，所以可以将分子中的绝对值符号去掉。所以有单个样本点到分割超平面的几何距离：<span class="arithmatex"><span class="MathJax_Preview">\gamma^{(i)} = \frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||}</span><script type="math/tex">\gamma^{(i)} = \frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||}</script></span></p>
<p>定义整个训练集到分割超平面的几何距离为离分割超平面最近的那个样本点到分割超平面的距离：<span class="arithmatex"><span class="MathJax_Preview">\gamma = \min_i \gamma^{(i)} = \min_i \frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||}</span><script type="math/tex">\gamma = \min_i \gamma^{(i)} = \min_i \frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||}</script></span></p>
<p>定义单个样本点到分割超平面的函数距离：<span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)} + b)</span><script type="math/tex">\hat{\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)} + b)</script></span></p>
<p>定义整个训练集到分割超平面的函数距离为离分割超平面最近的那个样本点到分割超平面的距离：<span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma} = \min_i \hat{\gamma}^{(i)} = \min_{i} y^{(i)}(w^Tx^{(i)}+b)</span><script type="math/tex">\hat{\gamma} = \min_i \hat{\gamma}^{(i)} = \min_{i} y^{(i)}(w^Tx^{(i)}+b)</script></span></p>
<p>至此我们定义了四个概念：</p>
<p>1.单个样本点到分割超平面的函数距离：<span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)} + b)</span><script type="math/tex">\hat{\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)} + b)</script></span></p>
<p>2.整个训练集到分割超平面的函数距离：<span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma} = \min_i \hat{\gamma}^{(i)}</span><script type="math/tex">\hat{\gamma} = \min_i \hat{\gamma}^{(i)}</script></span></p>
<p>3.单个样本点到分割超平面的几何距离：<span class="arithmatex"><span class="MathJax_Preview">\gamma^{(i)} = \frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||}</span><script type="math/tex">\gamma^{(i)} = \frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||}</script></span></p>
<p>4.整个训练集到分割超平面的几何距离：<span class="arithmatex"><span class="MathJax_Preview">\gamma = \min_i \gamma^{(i)}</span><script type="math/tex">\gamma = \min_i \gamma^{(i)}</script></span></p>
<p>对这四个概念涉及的性质，以及这四个概念与SVM算法之间联系的说明：</p>
<p>说明1.  SVM的直观想法是：在整个训练集中距离分割超平面最近的点，跟分割超平面的距离越远越好。用上我们上面的概念即为<span class="arithmatex"><span class="MathJax_Preview">\max \gamma</span><script type="math/tex">\max \gamma</script></span>，我们这里只是大概的表述一下，并不严谨。</p>
<p>说明2.  给定一个样本点和分割超平面，那么该样本点到分割超平面的几何距离就是确定的，不会改变。而该样本点到分割超平面的函数距离是可以变化的，这取决于我们怎样同等程度的改变w和b。比如样本点为<span class="arithmatex"><span class="MathJax_Preview">(x^{(k)}, y^{(k)})</span><script type="math/tex">(x^{(k)}, y^{(k)})</script></span>，分割超平面为<span class="arithmatex"><span class="MathJax_Preview">w^Tx+b=0</span><script type="math/tex">w^Tx+b=0</script></span>，那么函数距离：<span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}_{old} = y^{(k)}(w^Tx^{(k)} + b)</span><script type="math/tex">\hat{\gamma}_{old} = y^{(k)}(w^Tx^{(k)} + b)</script></span>；当我们把w变为2w，把b变为2b分割超平面就变成了<span class="arithmatex"><span class="MathJax_Preview">2w^Tx+2b=0</span><script type="math/tex">2w^Tx+2b=0</script></span>，很容易看出这实际上没有改变分割超平面，但函数距离变为了<span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}_{new} = y^{(k)}(2w^Tx^{(k)} + 2b) = 2\hat{\gamma}_{old}</span><script type="math/tex">\hat{\gamma}_{new} = y^{(k)}(2w^Tx^{(k)} + 2b) = 2\hat{\gamma}_{old}</script></span>。这是函数距离的一个很重要的性质，我们后面会用到。</p>
<p>说明3.  几何距离实际就是归一化后的函数距离，即<span class="arithmatex"><span class="MathJax_Preview">\gamma = \frac{\hat{\gamma}}{||w||}</span><script type="math/tex">\gamma = \frac{\hat{\gamma}}{||w||}</script></span></p>
<h3 id="13-svm">1.3 SVM推导<a class="headerlink" href="#13-svm" title="Permanent link">#</a></h3>
<p>用上前面定义的概念，我们来给出一个SVM问题的第一个严谨的描述：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\max_{w, b} \gamma</div>
<script type="math/tex; mode=display">\max_{w, b} \gamma</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">st. y^{(i)}(w^Tx^{(i)} + b) = \hat{\gamma}^{(i)} \ge \hat{\gamma}</div>
<script type="math/tex; mode=display">st. y^{(i)}(w^Tx^{(i)} + b) = \hat{\gamma}^{(i)} \ge \hat{\gamma}</script>
</div>
<p>说明：(1)最优化问题为使训练集到分割超平面的几何距离取得极大值；(2)训练集內的任何样本点到分割超平面的函数距离要大于等于训练集到分割超平面的函数距离。</p>
<p>令<span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma} = 1</span><script type="math/tex">\hat{\gamma} = 1</script></span>，有<span class="arithmatex"><span class="MathJax_Preview">\gamma = \frac{\hat{\gamma}}{||w||} = \frac{1}{||w||}</span><script type="math/tex">\gamma = \frac{\hat{\gamma}}{||w||} = \frac{1}{||w||}</script></span>，使用该式对SVM问题做变形得：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\max_{w,b} \gamma = \max_{w,b} \frac{1}{||w||}</div>
<script type="math/tex; mode=display">\max_{w,b} \gamma = \max_{w,b} \frac{1}{||w||}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">st. y^{(i)}(w^Tx^{(i)} + b) = \hat{\gamma}^{(i)} \ge 1</div>
<script type="math/tex; mode=display">st. y^{(i)}(w^Tx^{(i)} + b) = \hat{\gamma}^{(i)} \ge 1</script>
</div>
<p>说明：这里为什么可以令<span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}</span><script type="math/tex">\hat{\gamma}</script></span>等于1？在上文的“定义一些概念”部分的“说明2”中我们得出，在不改变样本点和分割超平面的情况下，通过给w和b同时乘以一个相同倍数的方法，我们可以使函数距离取得任意值，所以这里为了后续的计算方便，将函数距离<span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}</span><script type="math/tex">\hat{\gamma}</script></span>设为了1。</p>
<p>由于<span class="arithmatex"><span class="MathJax_Preview">\max \frac{1}{||w||}</span><script type="math/tex">\max \frac{1}{||w||}</script></span>与<span class="arithmatex"><span class="MathJax_Preview">\min ||w||</span><script type="math/tex">\min ||w||</script></span>效果是相同的，我们再做一下变形，另外为了计算方便，再取平方并乘以<span class="arithmatex"><span class="MathJax_Preview">\frac{1}{2}</span><script type="math/tex">\frac{1}{2}</script></span>，这些变化所得到的结果与原问题是等价的：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\min_{w,b} \frac{1}{2}||w||^2</div>
<script type="math/tex; mode=display">\min_{w,b} \frac{1}{2}||w||^2</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">st. y^{(i)}(w^Tx^{(i)} + b) \ge 1</div>
<script type="math/tex; mode=display">st. y^{(i)}(w^Tx^{(i)} + b) \ge 1</script>
</div>
<p>至此就推导出了我们的目标。</p>
<h2 id="lagrange-multiplier">二、拉格朗日乘子法（Lagrange Multiplier）<a class="headerlink" href="#lagrange-multiplier" title="Permanent link">#</a></h2>
<p>Primal Problem : </p>
<div class="arithmatex">
<div class="MathJax_Preview">\min_{w} f(w)</div>
<script type="math/tex; mode=display">\min_{w} f(w)</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">st. g_i(w) \leq 0，i=1,...,l</div>
<script type="math/tex; mode=display">st. g_i(w) \leq 0，i=1,...,l</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview"> h_i(w) = 0，i=1,...,k</div>
<script type="math/tex; mode=display"> h_i(w) = 0，i=1,...,k</script>
</div>
<p>定义拉格朗日乘子 : </p>
<div class="arithmatex">
<div class="MathJax_Preview">L(w,\alpha,\beta) = f(w) + \sum_{i=1}^{l}\alpha_ig_i(w) + \sum_{i=1}^{k}\beta_ih_i(w)</div>
<script type="math/tex; mode=display">L(w,\alpha,\beta) = f(w) + \sum_{i=1}^{l}\alpha_ig_i(w) + \sum_{i=1}^{k}\beta_ih_i(w)</script>
</div>
<p>做第一次变形（等价变形）：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\min_{w} \max_{\alpha,\beta;\alpha \ge 0} L(w,\alpha,\beta) = \min_{w} \max_{\alpha_i,\beta_i;\alpha_i \ge 0}[f(w) + \sum_{i=1}^{l} \alpha_i g_i(w) + \sum_{i=1}^k \beta_i h_i(w)]</div>
<script type="math/tex; mode=display">\min_{w} \max_{\alpha,\beta;\alpha \ge 0} L(w,\alpha,\beta) = \min_{w} \max_{\alpha_i,\beta_i;\alpha_i \ge 0}[f(w) + \sum_{i=1}^{l} \alpha_i g_i(w) + \sum_{i=1}^k \beta_i h_i(w)]</script>
</div>
<p>做第二次变形，即变为原来的对偶问题（原问题与本次的变形后的问题在一定条件下等价）：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\max_{\alpha,\beta;\alpha \ge 0} \min_{w} L(w, \alpha, \beta) = \max_{\alpha_i,\beta_i;\alpha_i \ge 0} \min_w [f(w) + \sum_{i=1}^l \alpha_i g_i(w) + \sum_{i=1}^k \beta_i h_i(w)]</div>
<script type="math/tex; mode=display">\max_{\alpha,\beta;\alpha \ge 0} \min_{w} L(w, \alpha, \beta) = \max_{\alpha_i,\beta_i;\alpha_i \ge 0} \min_w [f(w) + \sum_{i=1}^l \alpha_i g_i(w) + \sum_{i=1}^k \beta_i h_i(w)]</script>
</div>
<h2 id="_2">三、硬间隔和软间隔问题的求解<a class="headerlink" href="#_2" title="Permanent link">#</a></h2>
<h3 id="31">3.1 硬间隔<a class="headerlink" href="#31" title="Permanent link">#</a></h3>
<p>首先描述硬间隔问题，我们记为式(1)：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\min_{w,b} \frac{1}{2}||w||^2\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\min_{w,b} \frac{1}{2}||w||^2\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">st. \quad y^{(i)}(w^Tx^{(i)}+b) \ge 1</div>
<script type="math/tex; mode=display">st. \quad y^{(i)}(w^Tx^{(i)}+b) \ge 1</script>
</div>
<p>构造拉格朗日乘子：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}L(w,b,\alpha) = \frac{1}{2}||w||^2 + \sum_{i=1}^m \alpha_i [1-y^{(i)}(w^Tx^{(i)} + b)]\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}L(w,b,\alpha) = \frac{1}{2}||w||^2 + \sum_{i=1}^m \alpha_i [1-y^{(i)}(w^Tx^{(i)} + b)]\end{equation}</script>
</div>
<p>依据拉格朗日乘子法，我们有问题的转化关系：</p>
<div class="arithmatex">
<div class="MathJax_Preview">primal \quad problem \rightarrow \min_{w,b} \max_{\alpha} L(w,b,\alpha) \rightarrow \max_{\alpha} \min_{w,b}L(w,b,\alpha)</div>
<script type="math/tex; mode=display">primal \quad problem \rightarrow \min_{w,b} \max_{\alpha} L(w,b,\alpha) \rightarrow \max_{\alpha} \min_{w,b}L(w,b,\alpha)</script>
</div>
<p>先求解内层的极小值部分：<span class="arithmatex"><span class="MathJax_Preview">\min_{w,b} L(w,b,\alpha)</span><script type="math/tex">\min_{w,b} L(w,b,\alpha)</script></span>，即<span class="arithmatex"><span class="MathJax_Preview">\frac{\partial}{\partial w} L(w,b,\alpha) =0</span><script type="math/tex">\frac{\partial}{\partial w} L(w,b,\alpha) =0</script></span>，<span class="arithmatex"><span class="MathJax_Preview">\frac{\partial}{\partial b} L(w,b,\alpha) =0</span><script type="math/tex">\frac{\partial}{\partial b} L(w,b,\alpha) =0</script></span>，将这两个式子解出来得：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
&amp;\frac{\partial}{\partial w} L(w,b,\alpha) = w + \sum_{i=1}^m \alpha_i y^{(i)}x^{(i)} = 0 \Longrightarrow w=\sum_{i=1}^m \alpha_iy^{(i)}x^{(i)} \\
&amp;\frac{\partial}{\partial b} L(w,b,\alpha) = \sum_{i=1}^m \alpha_iy^{(i)} = 0 \Longrightarrow \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
&\frac{\partial}{\partial w} L(w,b,\alpha) = w + \sum_{i=1}^m \alpha_i y^{(i)}x^{(i)} = 0 \Longrightarrow w=\sum_{i=1}^m \alpha_iy^{(i)}x^{(i)} \\
&\frac{\partial}{\partial b} L(w,b,\alpha) = \sum_{i=1}^m \alpha_iy^{(i)} = 0 \Longrightarrow \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{split}\end{equation}</script>
</div>
<p>将关于<span class="arithmatex"><span class="MathJax_Preview">w, b</span><script type="math/tex">w, b</script></span>的解代入拉格朗日乘子中，并进行化简：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}L(w,b,\alpha)
&amp;= \frac{1}{2} w^Tw - \sum_{i=1}^m \alpha_i y^{(i)} w^Tx^{(i)} - b\sum_{i=1}^m \alpha_i y^{(i)} + \sum_{i=1}^m \alpha_i \\
&amp;= \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i\alpha_j y^{(i)}y^{(j)} x^{(i)}x^{(j)}
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}L(w,b,\alpha)
&= \frac{1}{2} w^Tw - \sum_{i=1}^m \alpha_i y^{(i)} w^Tx^{(i)} - b\sum_{i=1}^m \alpha_i y^{(i)} + \sum_{i=1}^m \alpha_i \\
&= \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i\alpha_j y^{(i)}y^{(j)} x^{(i)}x^{(j)}
\end{split}\end{equation}</script>
</div>
<p>到此我们把式(1)所描述的问题转为如下形式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\max_{\alpha_i} \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m}\alpha_i\alpha_j y^{(i)}y^{(j)} x^{(i)}x^{(j)}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\max_{\alpha_i} \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m}\alpha_i\alpha_j y^{(i)}y^{(j)} x^{(i)}x^{(j)}\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{split} st. \quad &amp;\sum_{i=1}^m \alpha_i y^{(i)} = 0 \\ &amp;\alpha_i \ge 0\end{split}</div>
<script type="math/tex; mode=display">\begin{split} st. \quad &\sum_{i=1}^m \alpha_i y^{(i)} = 0 \\ &\alpha_i \ge 0\end{split}</script>
</div>
<p>后面则是使用SMO算法解出<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>的值，然后通过<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>解出w和b，我们就获得了分割超平面。</p>
<h3 id="32">3.2 软间隔<a class="headerlink" href="#32" title="Permanent link">#</a></h3>
<p>首先描述软间隔问题，我们记为式(6)：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\min_{w,b} \frac{1}{2}||w||^2 + C\sum_{i=1}^m \xi_i\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\min_{w,b} \frac{1}{2}||w||^2 + C\sum_{i=1}^m \xi_i\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{split} st. \quad &amp;y^{(i)}(w^Tx^{(i)} + b) \ge 1 - \xi_i \\ &amp;\xi_i \ge 0 \end{split}</div>
<script type="math/tex; mode=display">\begin{split} st. \quad &y^{(i)}(w^Tx^{(i)} + b) \ge 1 - \xi_i \\ &\xi_i \ge 0 \end{split}</script>
</div>
<p>软间隔与硬间隔相比，只是多了一个变量<span class="arithmatex"><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span>，约束条件中的右侧减了一个<span class="arithmatex"><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span>意味着我们现在允许部分样本点到分割超平面的距离小于1（这个1就是我们在svm问题导出时定义的整个样本集到分割超平面的函数距离，我们之前把这个距离定为1）。同时在目标函数里加上所有的<span class="arithmatex"><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span>，这就意味着每有一个样本点距离分割超平面的距离小于1（当然有可能小于0）都会使得目标函数增大，这就保证了只有一部分样本点距离分割超平面的距离小于1，否则无法使得目标函数取得最小值。</p>
<p>软间隔的求解过程和硬间隔的求解过程类似，第一步是构造拉格朗日乘子：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}L(w,b,\xi, \alpha,\gamma) = \frac{1}{2}||w||^2 + C\sum_{i=1}^m \xi_i + \sum_{i=1}^m \alpha_i [1- \xi_i - y^{(i)}(w^Tx^{(i)} + b)] - \sum_{i=1}^m \gamma_i \xi_i\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}L(w,b,\xi, \alpha,\gamma) = \frac{1}{2}||w||^2 + C\sum_{i=1}^m \xi_i + \sum_{i=1}^m \alpha_i [1- \xi_i - y^{(i)}(w^Tx^{(i)} + b)] - \sum_{i=1}^m \gamma_i \xi_i\end{equation}</script>
</div>
<p>依照拉格朗日乘子法，我们有问题的转化关系：</p>
<div class="arithmatex">
<div class="MathJax_Preview">primal \quad problem \rightarrow \min_{w,b,\xi} \max_{\alpha,\gamma} L(w,b,\xi,\alpha,\gamma) \rightarrow \max_{\alpha,\gamma} \min_{w,b,\xi}L(w,b,\xi,\alpha,\gamma)</div>
<script type="math/tex; mode=display">primal \quad problem \rightarrow \min_{w,b,\xi} \max_{\alpha,\gamma} L(w,b,\xi,\alpha,\gamma) \rightarrow \max_{\alpha,\gamma} \min_{w,b,\xi}L(w,b,\xi,\alpha,\gamma)</script>
</div>
<p>接下来先求解内层的极小值部分：<span class="arithmatex"><span class="MathJax_Preview">\min_{w,b,\xi} L(w,b,\xi, \alpha, \gamma)</span><script type="math/tex">\min_{w,b,\xi} L(w,b,\xi, \alpha, \gamma)</script></span>，即<span class="arithmatex"><span class="MathJax_Preview">\frac{\partial}{\partial w}L(w,b,\xi,\alpha,\gamma)=0</span><script type="math/tex">\frac{\partial}{\partial w}L(w,b,\xi,\alpha,\gamma)=0</script></span>，<span class="arithmatex"><span class="MathJax_Preview">\frac{\partial}{\partial b}L(w,b,\xi,\alpha,\gamma) = 0</span><script type="math/tex">\frac{\partial}{\partial b}L(w,b,\xi,\alpha,\gamma) = 0</script></span>，<span class="arithmatex"><span class="MathJax_Preview">\frac{\partial}{\partial \xi} L(w,b,\xi,\alpha,\gamma) = 0</span><script type="math/tex">\frac{\partial}{\partial \xi} L(w,b,\xi,\alpha,\gamma) = 0</script></span>，将这三个式子解出来并记为式(8)：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
&amp;\frac{\partial}{\partial w}L(w,b,\xi,\alpha,\gamma) = w - \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)} = 0 \Longrightarrow w = \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)} \\
&amp;\frac{\partial}{\partial b}L(w,b,\xi,\alpha, \gamma) = \sum_{i=1}^m \alpha_i y^{(i)} = 0 \Longrightarrow \sum_{i=1}^m \alpha_i y^{(i)}=0 \\
&amp;\frac{\partial}{\partial \xi}L(w,b,\xi,\alpha,\gamma) = \sum_{i=1}^mC -\sum_{i=1}^m \alpha_i - \sum_{i=1}^m \gamma_i = 0 \Longrightarrow \sum_{i=1}^m (C - \alpha_i - \gamma_i) = 0\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
&\frac{\partial}{\partial w}L(w,b,\xi,\alpha,\gamma) = w - \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)} = 0 \Longrightarrow w = \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)} \\
&\frac{\partial}{\partial b}L(w,b,\xi,\alpha, \gamma) = \sum_{i=1}^m \alpha_i y^{(i)} = 0 \Longrightarrow \sum_{i=1}^m \alpha_i y^{(i)}=0 \\
&\frac{\partial}{\partial \xi}L(w,b,\xi,\alpha,\gamma) = \sum_{i=1}^mC -\sum_{i=1}^m \alpha_i - \sum_{i=1}^m \gamma_i = 0 \Longrightarrow \sum_{i=1}^m (C - \alpha_i - \gamma_i) = 0\end{split}\end{equation}</script>
</div>
<p>然后利用式(8)的三个解对拉格朗日乘子式(7)进行化简：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
L(w,b,\xi,\alpha,\gamma)
&amp;= \frac{1}{2}w^Tw + \sum_{i=1}^m \alpha_i - \sum_{i=1}^m \alpha_i\xi_i - \sum_{i=1}^m \alpha_iy^{(i)}w^Tx^{(i)} - b\sum_{i=1}^m \alpha_iy^{(i)} + C\sum_{i=1}^m \xi_i - \sum_{i=1}^m \gamma_i \xi_i \\
&amp;= \frac{1}{2}w^Tw + \sum_{i=1}^m \alpha_i - \sum_{i=1}^m \alpha_i\xi_i - \sum_{i=1}^m \alpha_iy^{(i)}w^Tx^{(i)} - b\sum_{i=1}^m \alpha_iy^{(i)} + \sum_{i=1}^m \alpha_i\xi_i \\
&amp;= \frac{1}{2}w^Tw + \sum_{i=1}^m \alpha_i - \sum_{i=1}^m \alpha_iy^{(i)}w^Tx^{(i)} \\
&amp;= \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i,j=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)} x^{(i)}x^{(i)}\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
L(w,b,\xi,\alpha,\gamma)
&= \frac{1}{2}w^Tw + \sum_{i=1}^m \alpha_i - \sum_{i=1}^m \alpha_i\xi_i - \sum_{i=1}^m \alpha_iy^{(i)}w^Tx^{(i)} - b\sum_{i=1}^m \alpha_iy^{(i)} + C\sum_{i=1}^m \xi_i - \sum_{i=1}^m \gamma_i \xi_i \\
&= \frac{1}{2}w^Tw + \sum_{i=1}^m \alpha_i - \sum_{i=1}^m \alpha_i\xi_i - \sum_{i=1}^m \alpha_iy^{(i)}w^Tx^{(i)} - b\sum_{i=1}^m \alpha_iy^{(i)} + \sum_{i=1}^m \alpha_i\xi_i \\
&= \frac{1}{2}w^Tw + \sum_{i=1}^m \alpha_i - \sum_{i=1}^m \alpha_iy^{(i)}w^Tx^{(i)} \\
&= \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i,j=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)} x^{(i)}x^{(i)}\end{split}\end{equation}</script>
</div>
<p>至此我们将式(6)的软间隔问题转化为如下形式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\max_{\alpha_i} \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)} x^{(i)}x^{(j)}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\max_{\alpha_i} \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)} x^{(i)}x^{(j)}\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{split} st. \quad &amp;\sum_{i=1}^m \alpha_i y^{(i)} = 0 \\ &amp;\sum_{i=1}^m (C-\alpha_i -\gamma_i) = 0 \\ &amp;\alpha_i \ge 0 \\ &amp;\gamma_i \ge 0 \end{split}</div>
<script type="math/tex; mode=display">\begin{split} st. \quad &\sum_{i=1}^m \alpha_i y^{(i)} = 0 \\ &\sum_{i=1}^m (C-\alpha_i -\gamma_i) = 0 \\ &\alpha_i \ge 0 \\ &\gamma_i \ge 0 \end{split}</script>
</div>
<p>接下来还是使用SMO算法求出<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>，再由<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>求出<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>、<span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span>、<span class="arithmatex"><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span>就确定了我们的分割超平面。</p>
<p>可以看出软间隔和硬间隔的求解过程是非常相似的。</p>
<h2 id="smo">四、SMO算法步骤总结<a class="headerlink" href="#smo" title="Permanent link">#</a></h2>
<h3 id="41">4.1 首先是待优化问题<a class="headerlink" href="#41" title="Permanent link">#</a></h3>
<div class="arithmatex">
<div class="MathJax_Preview">max_\alpha W(\alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i,j=1}^m y^{(i)}y^{(j)}\alpha_i\alpha_j&lt;x^{(i)}x^{(j)}&gt;</div>
<script type="math/tex; mode=display">max_\alpha W(\alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i,j=1}^m y^{(i)}y^{(j)}\alpha_i\alpha_j<x^{(i)}x^{(j)}></script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview"> st. 0 \leq \alpha_i \leq C , i = 1,2, ... ..., m</div>
<script type="math/tex; mode=display"> st. 0 \leq \alpha_i \leq C , i = 1,2, ... ..., m</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview"> \sum_{i=1}^m \alpha_i y^{(i)} = 0</div>
<script type="math/tex; mode=display"> \sum_{i=1}^m \alpha_i y^{(i)} = 0</script>
</div>
<p>然后是SMO算法的大体流程：
Repeat till convergence {</p>
<p>1、从<span class="arithmatex"><span class="MathJax_Preview">\alpha_1,\alpha_2, ... ... , \alpha_m</span><script type="math/tex">\alpha_1,\alpha_2, ... ... , \alpha_m</script></span>中选取出一对<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>进行优化；</p>
<p>2、将除了上一步中挑选的<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>以外的<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>看做常数，对选出的两个<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>进行更新使得<span class="arithmatex"><span class="MathJax_Preview">W(\alpha)</span><script type="math/tex">W(\alpha)</script></span>取得更大的值；</p>
<p>}</p>
<h3 id="42">4.2 算法流程中三个地方详细说明<a class="headerlink" href="#42" title="Permanent link">#</a></h3>
<p>1、循环的结束条件，即如何算是收敛。略。</p>
<p>2、如何选取<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>。略（这个现在没搞明白）。</p>
<p>3、如何更新<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>：</p>
<p>公式<span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^m \alpha_i y^{(i)}</span><script type="math/tex">\sum_{i=1}^m \alpha_i y^{(i)}</script></span>中除<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>以外<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>都被看做常数，所以我们将约束：</p>
<div class="arithmatex">
<div class="MathJax_Preview"> \sum_{i=1}^m \alpha_i y^{(i)} = 0</div>
<script type="math/tex; mode=display"> \sum_{i=1}^m \alpha_i y^{(i)} = 0</script>
</div>
<p>写成另一种形式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\alpha_j y^{(j)} + \alpha_k y^{(k)} + \zeta = 0</div>
<script type="math/tex; mode=display">\alpha_j y^{(j)} + \alpha_k y^{(k)} + \zeta = 0</script>
</div>
<p>再做一点变形即<span class="arithmatex"><span class="MathJax_Preview">\alpha_jy^{(j)} + \alpha_ky^{(k)} = -\zeta</span><script type="math/tex">\alpha_jy^{(j)} + \alpha_ky^{(k)} = -\zeta</script></span>，其中<span class="arithmatex"><span class="MathJax_Preview">\zeta</span><script type="math/tex">\zeta</script></span>表示一个常数。</p>
<p>我们将<span class="arithmatex"><span class="MathJax_Preview">\zeta</span><script type="math/tex">\zeta</script></span>前面的负号去掉（<span class="arithmatex"><span class="MathJax_Preview">\zeta</span><script type="math/tex">\zeta</script></span>是我们自己定义的一个常数，这里负号去掉并没什么关系）然后解出<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>：<span class="arithmatex"><span class="MathJax_Preview">\alpha_j = \frac{\zeta - \alpha_k y^{(k)}}{y^{(j)}}</span><script type="math/tex">\alpha_j = \frac{\zeta - \alpha_k y^{(k)}}{y^{(j)}}</script></span></p>
<p>所以此时的<span class="arithmatex"><span class="MathJax_Preview">W(\alpha)</span><script type="math/tex">W(\alpha)</script></span>是只关于<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>的函数（因为<span class="arithmatex"><span class="MathJax_Preview">W(\alpha)</span><script type="math/tex">W(\alpha)</script></span>中的参数<span class="arithmatex"><span class="MathJax_Preview">\alpha_1,...,\alpha_m</span><script type="math/tex">\alpha_1,...,\alpha_m</script></span>中除<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>外都是常数，<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>也可以用<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>表示）。</p>
<p>所以原优化问题变成了：</p>
<div class="arithmatex">
<div class="MathJax_Preview">max_{\alpha_k} W(\alpha_k)</div>
<script type="math/tex; mode=display">max_{\alpha_k} W(\alpha_k)</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">st. 0 \leq \alpha_k \leq C</div>
<script type="math/tex; mode=display">st. 0 \leq \alpha_k \leq C</script>
</div>
<p>从原来的优化问题的<span class="arithmatex"><span class="MathJax_Preview">W(\alpha)</span><script type="math/tex">W(\alpha)</script></span>表达式中可以看出，<span class="arithmatex"><span class="MathJax_Preview">W(\alpha)</span><script type="math/tex">W(\alpha)</script></span>中<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>的最高次幂为2，所以我们的新优化问题中<span class="arithmatex"><span class="MathJax_Preview">max_{\alpha_k} W(\alpha_k)</span><script type="math/tex">max_{\alpha_k} W(\alpha_k)</script></span>实际是一个关于<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>的二次函数，对于自变量<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>有个限制条件是必须要在0到C之间，这个新优化问题应该绝大部分人都会求了。</p>
<p>求出<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>，再利用<span class="arithmatex"><span class="MathJax_Preview">\alpha_j = \frac{\zeta - \alpha_k y^{(k)}}{y^{(j)}}</span><script type="math/tex">\alpha_j = \frac{\zeta - \alpha_k y^{(k)}}{y^{(j)}}</script></span>求出<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>，这样就求出了新的<span class="arithmatex"><span class="MathJax_Preview">\alpha_j</span><script type="math/tex">\alpha_j</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>，替换原来的值就可以了。</p>
<h3 id="43">4.3 总结<a class="headerlink" href="#43" title="Permanent link">#</a></h3>
<p>对<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>个<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>初始化一下，比如说全赋值为0。然后选取想要进行更新的两个<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>，然后用上面的第三步对这两个<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>进行更新。最后一直循环执行更新<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>的步骤，直到我们使<span class="arithmatex"><span class="MathJax_Preview">W(\alpha)</span><script type="math/tex">W(\alpha)</script></span>取得极值。</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
