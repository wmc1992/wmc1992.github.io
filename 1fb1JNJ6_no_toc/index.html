<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/006_LLM/004_%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84/04_SwiGLU/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Index - 算法工程师笔记</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../../css/extra.css" rel="stylesheet">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-12" role="main">

<p align="right">[<a href="/1fb1JNJ6_no_toc/">隐藏左侧目录栏</a>][<a href="/1fb1JNJ6/">显示左侧目录栏</a>]</p>

<h2 id="glu-swiglu">GLU 和 SwiGLU<a class="headerlink" href="#glu-swiglu" title="Permanent link">#</a></h2>
<p>PaLM 和 LLaMA 中都使用 SwiGLU 替换了 FFN，本文介绍一下 GLU 和 SwiGLU。</p>
<blockquote>
<p>GLU论文链接: <a href="https://arxiv.org/pdf/1612.08083.pdf">https://arxiv.org/pdf/1612.08083.pdf</a></p>
<p>SwiGLU论文链接: <a href="https://arxiv.org/pdf/2002.05202.pdf">https://arxiv.org/pdf/2002.05202.pdf</a></p>
<p>PaLM论文链接: <a href="https://arxiv.org/pdf/2204.02311.pdf">https://arxiv.org/pdf/2204.02311.pdf</a></p>
<p>LLaMA论文链接: <a href="https://arxiv.org/pdf/2302.13971.pdf">https://arxiv.org/pdf/2302.13971.pdf</a></p>
</blockquote>
<p><span></span></p>
<blockquote>
<p>关于符号的说明：本文对应着两篇论文，一篇是GLU，另一篇是SwiGLU。在下文中描述GLU和SwiGLU时各自使用对应论文中的符号，所以有可能出现符号不一致的问题。</p>
</blockquote>
<h2 id="1glu">1、GLU<a class="headerlink" href="#1glu" title="Permanent link">#</a></h2>
<blockquote>
<p>出自2017年的论文 <a href="https://arxiv.org/pdf/1612.08083.pdf">Language Modeling with Gated Convolutional Networks</a></p>
</blockquote>
<p>GLU 全称为 Gated Linear Unit，即门控线性单元函数。在原始论文中使用下图1来解释该函数，下图是一个完整的网络结构，其中 "Input Sentence"、"Lookup Table" 都是输入层，"Softmax" 是输出层，这两部分都不考虑。接下来主要看的是下图的 "Convolution" 和 "Gating" 这两层，这两层对应的就是 GLU。</p>
<p>首先输入向量<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>（在下图中使用的符号是<span class="arithmatex"><span class="MathJax_Preview">E</span><script type="math/tex">E</script></span>）经过两个独立的卷积层/MLP层，得到向量<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>和向量<span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span>。此时，向量<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>和向量<span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span>的公式为：<span class="arithmatex"><span class="MathJax_Preview">A=x \cdot W + b</span><script type="math/tex">A=x \cdot W + b</script></span>，<span class="arithmatex"><span class="MathJax_Preview">B=x \cdot V + c</span><script type="math/tex">B=x \cdot V + c</script></span>。然后向量<span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span>经过一个sigmoid函数之后，<span class="arithmatex"><span class="MathJax_Preview">\sigma(B)</span><script type="math/tex">\sigma(B)</script></span>中的每个元素就都变为了0～1之间的值，就可以起到控制信息是否通过的作用（下图画的是向量<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>经过sigmoid函数，这是有点问题的）。</p>
<p>将向量<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>与<span class="arithmatex"><span class="MathJax_Preview">\sigma(B)</span><script type="math/tex">\sigma(B)</script></span>逐个元素相乘之后就得到了GLU层的最终输出结果，把上面描述的整个过程结合起来，GLU的公式如下：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
\text{GLU}(x) &amp;=A \otimes \sigma(B) \\
&amp;= (x \cdot W + b) \otimes \sigma(x \cdot V + c)
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
\text{GLU}(x) &=A \otimes \sigma(B) \\
&= (x \cdot W + b) \otimes \sigma(x \cdot V + c)
\end{split}\end{equation}</script>
</div>
<p>在该公式中，<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 表示输入向量，<span class="arithmatex"><span class="MathJax_Preview">\otimes</span><script type="math/tex">\otimes</script></span> 表示两个向量逐元素相乘，<span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> 表示sigmoid函数。</p>
<table align=center style="width:50%">
    <thead>
        <tr>
            <th>图1</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td align="center"><img src="/1fb1JNJ6/assets/01.png" width=100%/></td>
        </tr>
    </tbody>
</table>

<p>公式（1）是原论文中提出的公式形式，当GLU作为激活函数时一般不是上述公式（1）的形式。激活函数 ReLU 的公式为 <span class="arithmatex"><span class="MathJax_Preview">\text{ReLU} = \max(0, x)</span><script type="math/tex">\text{ReLU} = \max(0, x)</script></span>，其含义就是当输入向量 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 的值小于 0 时直接阻断，当时输入向量 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 的值大于 0 值直接通过。参考ReLU激活函数，激活函数GLU的公式为如下公式（2）的形式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{GLU}(x) = x \otimes \sigma (g(x))\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{GLU}(x) = x \otimes \sigma (g(x))\end{equation}</script>
</div>
<p>这里有一个新符号 <span class="arithmatex"><span class="MathJax_Preview">g(x)</span><script type="math/tex">g(x)</script></span> 表示的是向量 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 经过一层MLP或者卷积层，其他部分的符号与公式（1）中是相同的。可以看出公式（2）的右半部分与公式（1）的右半部分是完全一样的，所不同的是公式（2）中的左半部分直接就是向量 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>，这和上面描述的激活函数ReLU是相似的，当 <span class="arithmatex"><span class="MathJax_Preview">\sigma(g(x))</span><script type="math/tex">\sigma(g(x))</script></span> 趋近于 0 时表示对 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 进行阻断，当 <span class="arithmatex"><span class="MathJax_Preview">\sigma(g(x))</span><script type="math/tex">\sigma(g(x))</script></span> 趋近于 1 时表示允许 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 通过，以此实现门控激活函数的效果。</p>
<h2 id="2swiglu">2、SwiGLU<a class="headerlink" href="#2swiglu" title="Permanent link">#</a></h2>
<h3 id="21-ffn">2.1 FFN公式及其变体<a class="headerlink" href="#21-ffn" title="Permanent link">#</a></h3>
<p>我们知道FFN就是输入向量<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>先经过一个MLP层，将维度上升到原来的4倍，然后过一个激活函数层，最后再经过一个MLP层，将维度还原回去。其公式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{FFN}(x, W_1, W_2, b_1, b_2) = \text{ReLU}(xW_1+b_1)W_2+b_2\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{FFN}(x, W_1, W_2, b_1, b_2) = \text{ReLU}(xW_1+b_1)W_2+b_2\end{equation}</script>
</div>
<p>有些研究，比如T5，将实验中的偏置项去掉了，这样FFN就变为了如下形式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{FFN}_{\text{ReLU}}(x, W_1, W_2, b_1, b_2) = \text{ReLU}(xW_1)W_2\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{FFN}_{\text{ReLU}}(x, W_1, W_2, b_1, b_2) = \text{ReLU}(xW_1)W_2\end{equation}</script>
</div>
<p>把这里的激活函数由 ReLU 替换为 GeLU 或 Swish 之后，就得到两种新的FFN，公式如下，下面这两种也都有相应的实验结果。</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{FFN}_{\text{GeLU}}(x, W_1, W_2, b_1, b_2) = \text{GeLU}(xW_1)W_2\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{FFN}_{\text{GeLU}}(x, W_1, W_2, b_1, b_2) = \text{GeLU}(xW_1)W_2\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{FFN}_{\text{Swish}}(x, W_1, W_2, b_1, b_2) = \text{Swish}(xW_1)W_2\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{FFN}_{\text{Swish}}(x, W_1, W_2, b_1, b_2) = \text{Swish}(xW_1)W_2\end{equation}</script>
</div>
<blockquote>
<p>从这里来看，深度学习还是穷举各种情况挨个试一遍：ReLU、GeLU、Swish，以及本文的 GLU。</p>
</blockquote>
<h3 id="22-glu">2.2 GLU公式及其变体<a class="headerlink" href="#22-glu" title="Permanent link">#</a></h3>
<p>上一小节中的公式（1）为原始的 GLU 公式，在这里我们重新写一遍，如下：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{GLU}(x, W, V, b, c)=\sigma(xW+b) \otimes (xV+c)\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{GLU}(x, W, V, b, c)=\sigma(xW+b) \otimes (xV+c)\end{equation}</script>
</div>
<p>在该公式中，左侧部分有一个Sigmoid函数，把这个函数替换为其他函数就可以得到GLU函数的各种变体，下面是其一系列变体。公式（8）是直接把Sigmoid函数去掉，公式（9）、（10）、（11）是分别把Sigmoid函数替换为ReLU、GeLU、Swish：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{Bilinear}(x, W, V, b, c)=(xW+b) \otimes (xV+c)\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{Bilinear}(x, W, V, b, c)=(xW+b) \otimes (xV+c)\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{ReGLU}(x, W, V, b, c)=\text{ReLU}(xW+b) \otimes (xV+c)\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{ReGLU}(x, W, V, b, c)=\text{ReLU}(xW+b) \otimes (xV+c)\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{GEGLU}(x, W, V, b, c)=\text{GELU}(xW+b) \otimes (xV+c)\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{GEGLU}(x, W, V, b, c)=\text{GELU}(xW+b) \otimes (xV+c)\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{SwiGLU}(x, W, V, b, c, \beta)=\text{Swish}_{\beta}(xW+b) \otimes (xV+c)\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{SwiGLU}(x, W, V, b, c, \beta)=\text{Swish}_{\beta}(xW+b) \otimes (xV+c)\end{equation}</script>
</div>
<p>接下来把所有的偏置项都给去掉，就得到下述五个公式，大同小异没什么本质区别：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
&amp;\text{GLU}(x, W, V)=\sigma(xW) \otimes (xV) \\
&amp;\text{Bilinear}(x, W, V)=(xW) \otimes (xV) \\
&amp;\text{ReGLU}(x, W, V)=\text{ReLU}(xW) \otimes (xV) \\
&amp;\text{GEGLU}(x, W, V)=\text{GELU}(xW) \otimes (xV) \\
&amp;\text{SwiGLU}(x, W, V, \beta)=\text{Swish}_{\beta}(xW) \otimes (xV)
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
&\text{GLU}(x, W, V)=\sigma(xW) \otimes (xV) \\
&\text{Bilinear}(x, W, V)=(xW) \otimes (xV) \\
&\text{ReGLU}(x, W, V)=\text{ReLU}(xW) \otimes (xV) \\
&\text{GEGLU}(x, W, V)=\text{GELU}(xW) \otimes (xV) \\
&\text{SwiGLU}(x, W, V, \beta)=\text{Swish}_{\beta}(xW) \otimes (xV)
\end{split}\end{equation}</script>
</div>
<h3 id="23-ffnglu">2.3 将FFN的变体与GLU的变体结合<a class="headerlink" href="#23-ffnglu" title="Permanent link">#</a></h3>
<p>使用公式（12）中的 GLU 及其各种变体替换掉 FFN 中的第一个MLP和激活函数，就可以得到如下新版的 FFN 公式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
&amp;\text{FFN}_{\text{GLU}}(x, W, V)=\big[\sigma(xW) \otimes (xV)\big] W_2 \\
&amp;\text{FFN}_{\text{Bilinear}}(x, W, V)=\big[(xW) \otimes (xV)\big] W_2 \\
&amp;\text{FFN}_{\text{ReGLU}}(x, W, V)=\big[\text{ReLU}(xW) \otimes (xV)\big] W_2 \\
&amp;\text{FFN}_{\text{GEGLU}}(x, W, V)=\big[\text{GELU}(xW) \otimes (xV)\big] W_2 \\
&amp;\text{FFN}_{\text{SwiGLU}}(x, W, V)=\big[\text{Swish}(xW) \otimes (xV)\big] W_2
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
&\text{FFN}_{\text{GLU}}(x, W, V)=\big[\sigma(xW) \otimes (xV)\big] W_2 \\
&\text{FFN}_{\text{Bilinear}}(x, W, V)=\big[(xW) \otimes (xV)\big] W_2 \\
&\text{FFN}_{\text{ReGLU}}(x, W, V)=\big[\text{ReLU}(xW) \otimes (xV)\big] W_2 \\
&\text{FFN}_{\text{GEGLU}}(x, W, V)=\big[\text{GELU}(xW) \otimes (xV)\big] W_2 \\
&\text{FFN}_{\text{SwiGLU}}(x, W, V)=\big[\text{Swish}(xW) \otimes (xV)\big] W_2
\end{split}\end{equation}</script>
</div>
<p>接下来就是做实验验证上述各个新版的FFN与原版的FFN究竟哪个效果更好了。就目前来看，PaLM 和 LLaMA 中都是使用的 <span class="arithmatex"><span class="MathJax_Preview">\text{FFN}_{\text{SwiGLU}}</span><script type="math/tex">\text{FFN}_{\text{SwiGLU}}</script></span>。</p>
<h3 id="24-swiglu">2.4 SwiGLU替换之后的维度<a class="headerlink" href="#24-swiglu" title="Permanent link">#</a></h3>
<p>原来的FFN有两个MLP层，这两个MLP层的参数量分别为：<span class="arithmatex"><span class="MathJax_Preview">h \times 4h</span><script type="math/tex">h \times 4h</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">4h \times h</span><script type="math/tex">4h \times h</script></span>，总的参数量为 <span class="arithmatex"><span class="MathJax_Preview">8h^2</span><script type="math/tex">8h^2</script></span>。</p>
<p>SwiGLU 的公式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\text{FFN}_{\text{SwiGLU}}(x, W, V)=\big[\text{Swish}(xW) \otimes (xV)\big] W_2</div>
<script type="math/tex; mode=display">\text{FFN}_{\text{SwiGLU}}(x, W, V)=\big[\text{Swish}(xW) \otimes (xV)\big] W_2</script>
</div>
<p>从上述公式中可以知道，矩阵 <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 与矩阵 <span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> 的维度是相同的，其作用是对输入向量 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 进行升维；矩阵 <span class="arithmatex"><span class="MathJax_Preview">W_2</span><script type="math/tex">W_2</script></span> 的作用是将高维的隐向量还原到和输入向量 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 相同的维度。所以 <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>、<span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span>、<span class="arithmatex"><span class="MathJax_Preview">W_2</span><script type="math/tex">W_2</script></span> 这三个矩阵的维度分别为：<span class="arithmatex"><span class="MathJax_Preview">(h, \alpha h)</span><script type="math/tex">(h, \alpha h)</script></span>、<span class="arithmatex"><span class="MathJax_Preview">(h, \alpha h)</span><script type="math/tex">(h, \alpha h)</script></span>、<span class="arithmatex"><span class="MathJax_Preview">(\alpha h, h)</span><script type="math/tex">(\alpha h, h)</script></span>，总的参数量为 <span class="arithmatex"><span class="MathJax_Preview">3\alpha h^2</span><script type="math/tex">3\alpha h^2</script></span>。为了保持和原始的 FFN 参数量相同，有：</p>
<div class="arithmatex">
<div class="MathJax_Preview">8h^2 = 3 \alpha h^2</div>
<script type="math/tex; mode=display">8h^2 = 3 \alpha h^2</script>
</div>
<p>解得 <span class="arithmatex"><span class="MathJax_Preview">\alpha=\frac{8}{3}</span><script type="math/tex">\alpha=\frac{8}{3}</script></span>，最终 <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>、<span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span>、<span class="arithmatex"><span class="MathJax_Preview">W_2</span><script type="math/tex">W_2</script></span> 这三个矩阵的维度分别为：<span class="arithmatex"><span class="MathJax_Preview">(h, \frac{8}{3} h)</span><script type="math/tex">(h, \frac{8}{3} h)</script></span>、<span class="arithmatex"><span class="MathJax_Preview">(h, \frac{8}{3} h)</span><script type="math/tex">(h, \frac{8}{3} h)</script></span>、<span class="arithmatex"><span class="MathJax_Preview">(\frac{8}{3} h, h)</span><script type="math/tex">(\frac{8}{3} h, h)</script></span>，可以很明显的看出严格按照该公式计算出来的不是整数，所以使用该公式计算出来的是模型真实维度的近似值。</p>
<p>下面是65B的LLaMA的模型结构，可以看到在attention部分隐藏层的维度为8192，乘上 <span class="arithmatex"><span class="MathJax_Preview">\frac{8}{3}</span><script type="math/tex">\frac{8}{3}</script></span> 就是 <span class="arithmatex"><span class="MathJax_Preview">8192 * \frac{8}{3} = 21845.33</span><script type="math/tex">8192 * \frac{8}{3} = 21845.33</script></span>，与下述模型中真实的维度 22016 是基本一致的。</p>
<pre><code>LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 8192, padding_idx=0)
    (layers): ModuleList(
      (0-79): 80 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)
          (k_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)
          (v_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)
          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)
          (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)
          (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)
)
</code></pre>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">#</a></h2>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/486055017">https://zhuanlan.zhihu.com/p/486055017</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/486798318">https://zhuanlan.zhihu.com/p/486798318</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1612.08083.pdf">https://arxiv.org/pdf/1612.08083.pdf</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2002.05202.pdf">https://arxiv.org/pdf/2002.05202.pdf</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2204.02311.pdf">https://arxiv.org/pdf/2204.02311.pdf</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2302.13971.pdf">https://arxiv.org/pdf/2302.13971.pdf</a></p>
</li>
</ul></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="../../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
