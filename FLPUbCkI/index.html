<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/003_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/002_%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Index - 算法工程师笔记</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../css/extra.css" rel="stylesheet">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#_1" class="nav-link">激活函数</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#1" class="nav-link">1、激活函数的作用</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#2" class="nav-link">2、梯度爆炸和梯度消失</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#3sigmoid" class="nav-link">3、Sigmoid</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#4tanh" class="nav-link">4、tanh</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#5relu" class="nav-link">5、ReLU 系列激活函数</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#51-relu" class="nav-link">5.1 ReLU</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#52-leaky-relu" class="nav-link">5.2 Leaky ReLU</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#53-prelu-rrelu" class="nav-link">5.3 PReLU, RReLU</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#6gelu" class="nav-link">6、GELU</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#61-intuition" class="nav-link">6.1 Intuition</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#62" class="nav-link">6.2 函数以及导数图像</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#63" class="nav-link">6.3 加速计算</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#7glu" class="nav-link">7、GLU</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#8swish" class="nav-link">8、Swish</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#reference" class="nav-link">Reference</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p align="right">[<a href="/FLPUbCkI_no_toc/">隐藏左侧目录栏</a>][<a href="/FLPUbCkI/">显示左侧目录栏</a>]</p>

<h1 id="_1">激活函数<a class="headerlink" href="#_1" title="Permanent link">#</a></h1>
<h2 id="1">1、激活函数的作用<a class="headerlink" href="#1" title="Permanent link">#</a></h2>
<p>神经网络是线性的，无法解决非线性的问题，加入激活函数就是给模型引入非线性能力；</p>
<p>不同的激活函数，特点和作用不同：</p>
<ul>
<li>Sigmoid和tanh的特点是将输出限制在(0,1)和(-1,1)之间，说明Sigmoid和tanh适合做概率值的处理，例如LSTM中的各种门；而ReLU就不行，因为ReLU无最大值限制，可能会出现很大值。</li>
<li>Relu适合用于深层网络的训练，而Sigmoid和tanh则不行，因为它们会出现梯度消失。</li>
</ul>
<h2 id="2">2、梯度爆炸和梯度消失<a class="headerlink" href="#2" title="Permanent link">#</a></h2>
<p>模型中的梯度爆炸和梯度消失问题：</p>
<ol>
<li>激活函数导致的梯度消失，像 sigmoid 和 tanh 都会导致梯度消失；</li>
<li>矩阵连乘也会导致梯度消失，这个原因导致的梯度消失无法通过更换激活函数来避免；该问题的具体解释见 <a href="https://www.zhihu.com/question/49230360/answer/1490257443">https://www.zhihu.com/question/49230360/answer/1490257443</a>。直观的说就是在反向传播时，梯度会连乘，当梯度都小于1.0时，就会出现梯度消失；当梯度都大于1.0时，就会出现梯度爆炸。</li>
</ol>
<p>如何解决梯度爆炸和梯度消失问题：</p>
<ol>
<li>上述第一个问题只需要使用像 ReLU 这种激活函数就可以解决；</li>
<li>上述第二个问题没有能够完全解决的方法，目前有一些方法可以很大程度上进行缓解该问题，比如：对梯度做截断解决梯度爆炸问题、残差连接、normalize。由于使用了残差连接和 normalize 之后梯度消失和梯度爆炸已经极少出现了，所以目前可以认为该问题已经解决了。</li>
</ol>
<h2 id="3sigmoid">3、Sigmoid<a class="headerlink" href="#3sigmoid" title="Permanent link">#</a></h2>
<p>Sigmoid 函数公式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\sigma(z) = \frac{1}{1 + e^{-z}}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\sigma(z) = \frac{1}{1 + e^{-z}}\end{equation}</script>
</div>
<p>导数公式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\sigma^\prime(z) = \sigma(z)(1 - \sigma(z))\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\sigma^\prime(z) = \sigma(z)(1 - \sigma(z))\end{equation}</script>
</div>
<table>
<thead>
<tr>
<th>图1: sigmoid函数图像</th>
<th>图2: sigmoid导数图像</th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="" src="/FLPUbCkI/assets/01.png" /></td>
<td><img alt="" src="/FLPUbCkI/assets/02.png" /></td>
</tr>
</tbody>
</table>
<p>优点：</p>
<ul>
<li>平滑，易于求导；</li>
<li>取值范围是(0, 1)，可直接用于求概率值的问题或者分类问题；比如 LSTM 中的门，二分类或者多标签分类问题；</li>
</ul>
<p>缺点：</p>
<ul>
<li>计算量大，包含幂运算，以及除法运算；</li>
<li>sigmoid 导数的取值范围是 [0, 0.25]，最大值都是小于 1 的，反向传播时又是"链式传导"，经过几次相乘之后很容易就会出现梯度消失的问题；</li>
<li>sigmoid 的输出的均值不是0（即zero-centered），这会导致当前层接收到上一层的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布；</li>
</ul>
<h2 id="4tanh">4、tanh<a class="headerlink" href="#4tanh" title="Permanent link">#</a></h2>
<p>tanh 的函数公式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
\tanh (z) &amp;= \frac{e^z - e^{-z}}{e^z + e^{-z}} \\
&amp;= \frac{2}{1 + e^{-2z}} - 1
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
\tanh (z) &= \frac{e^z - e^{-z}}{e^z + e^{-z}} \\
&= \frac{2}{1 + e^{-2z}} - 1
\end{split}\end{equation}</script>
</div>
<p>从上述公式的第二行可以看出，tanh 函数可以由 sigmoid 函数经过平移和拉伸得到。tanh 函数的取值范围是（-1, 1）。</p>
<table>
<thead>
<tr>
<th>图3: tanh函数图像</th>
<th>图4: tanh导数图像</th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="" src="/FLPUbCkI/assets/03.png" /></td>
<td><img alt="" src="/FLPUbCkI/assets/04.png" /></td>
</tr>
</tbody>
</table>
<p>tanh 函数可以理解为是基于 sigmoid 函数的一种改进的激活函数，所以对于 sigmoid 函数的缺点，它能够解决一部分。但是 tanh 函数依然有着不少的缺点。tanh 函数的特点如下：</p>
<ul>
<li>它的输出范围是(-1, 1)，解决了 sigmoid 函数输出的均值不是0（zero-centered）的问题；</li>
<li>tanh 的导数取值范围是(0, 1)，可以看出其在反向传播的"链式传导"过程中的梯度消失问题要比 sigmoid 函数要好一些，但是其依然存在着梯度消失问题；</li>
<li>幂运算依然存在，计算量比较大；</li>
</ul>
<h2 id="5relu">5、ReLU 系列激活函数<a class="headerlink" href="#5relu" title="Permanent link">#</a></h2>
<h3 id="51-relu">5.1 ReLU<a class="headerlink" href="#51-relu" title="Permanent link">#</a></h3>
<p>Relu 全称为 Rectified Linear Unit，即修正线性单元函数。该函数的公式比较简单，相应的公式和图像如下表所示。</p>
<table>
<thead>
<tr>
<th align="center">图5: ReLU函数</th>
<th align="center">图6: 导数</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><span class="arithmatex"><span class="MathJax_Preview">\text{ReLU}(z) = \begin{cases}0 &amp; \text{if } z \leqslant 0 \\z &amp; \text{if } z &gt; 0\end{cases}</span><script type="math/tex">\text{ReLU}(z) = \begin{cases}0 & \text{if } z \leqslant 0 \\z & \text{if } z > 0\end{cases}</script></span></td>
<td align="center"><span class="arithmatex"><span class="MathJax_Preview">\text{ReLU}^\prime (z) = \begin{cases}0 &amp; \text{if } z \leqslant 0 \\1 &amp; \text{if } z &gt; 0\end{cases}</span><script type="math/tex">\text{ReLU}^\prime (z) = \begin{cases}0 & \text{if } z \leqslant 0 \\1 & \text{if } z > 0\end{cases}</script></span></td>
</tr>
<tr>
<td align="center"><div align=center><img src="/FLPUbCkI/assets/05.png" width=70% /></div></td>
<td align="center"><div align=center><img src="/FLPUbCkI/assets/06.png" width=70% /></div></td>
</tr>
</tbody>
</table>
<p>相比于 sigmoid、tanh 这两个激活函数，ReLU 激活函数的优缺点如下：</p>
<ul>
<li>
<p>当 <span class="arithmatex"><span class="MathJax_Preview">z&gt;0</span><script type="math/tex">z>0</script></span> 时，ReLU 激活函数的导数恒为常数1，这就避免了 sigmoid 和 tanh 会在神经网络层数比较深的时候出现的梯度消失的问题；</p>
</li>
<li>
<p>计算复杂度低，不再含有幂运算，只需要一个阈值就能够得到其导数；</p>
</li>
<li>
<p>经过实际实验发现，使用 ReLU 作为激活函数，模型收敛的速度比 sigmoid 和 tanh 快；</p>
</li>
<li>
<p>当 <span class="arithmatex"><span class="MathJax_Preview">z&lt;0</span><script type="math/tex">z<0</script></span> 时，ReLU 激活函数的导数恒为常数0，这既带来了一些有利的方面，也导致了一些坏的方面，分别进行描述。</p>
<ul>
<li>
<p>有利的方面：在深度学习中，目标是从大量数据中学习到关键特征，也就是把密集矩阵转化为稀疏矩阵，保留数据的关键信息，去除噪音，这样的模型就有了鲁棒性。ReLU 激活函数中将 <span class="arithmatex"><span class="MathJax_Preview">z&lt;0</span><script type="math/tex">z<0</script></span> 的部分置为0，就是产生稀疏矩阵的过程。</p>
</li>
<li>
<p>坏的方面：将 <span class="arithmatex"><span class="MathJax_Preview">z&lt;0</span><script type="math/tex">z<0</script></span> 的部分梯度直接置为0会导致 Dead ReLU Problem(神经元坏死现象)。可能会导致部分神经元不再对输入数据做响应，无论输入什么数据，该部分神经元的参数都不会被更新。（这个问题是一个非常严重的问题，后续不少工作都是在解决这个问题）</p>
</li>
</ul>
</li>
<li>
<p>ReLU 有可能会导致梯度爆炸问题，解决方法是梯度截断；</p>
</li>
<li>
<p>ReLU 的输出不是 0 均值的，这个和 sigmoid 类似。（后续的优化工作 ELU 在该问题上解决的比较好，ELU 的输出是近似为0的）</p>
</li>
</ul>
<h3 id="52-leaky-relu">5.2 Leaky ReLU<a class="headerlink" href="#52-leaky-relu" title="Permanent link">#</a></h3>
<p>为了解决 ReLU 的 Dead ReLU 问题，提出了 渗漏整流线性单元(Leaky ReLU)，该方法是 ReLU 的一个变体。其在 <span class="arithmatex"><span class="MathJax_Preview">z&gt;0</span><script type="math/tex">z>0</script></span> 的部分与 ReLU 一样保持不变；在 <span class="arithmatex"><span class="MathJax_Preview">z&lt;0</span><script type="math/tex">z<0</script></span> 的部分，采用一个非常小的斜率 0.01，其公式如下：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\text{Leaky ReLU}(z) = \begin{cases}0.01 z &amp; \text{if } z \leqslant 0 \\z &amp; \text{if } z &gt; 0\end{cases}</div>
<script type="math/tex; mode=display">\text{Leaky ReLU}(z) = \begin{cases}0.01 z & \text{if } z \leqslant 0 \\z & \text{if } z > 0\end{cases}</script>
</div>
<p>其图像如下所示：</p>
<div align=center><img src="/FLPUbCkI/assets/07.png" width=30% /></div>

<p>该方法是 ReLU 的一个变体，能够在一定程度上解决 Dead ReLU 问题，但是该方法的缺点是效果并不稳定，所以实际实验中使用该方法的并不多。</p>
<h3 id="53-prelu-rrelu">5.3 PReLU, RReLU<a class="headerlink" href="#53-prelu-rrelu" title="Permanent link">#</a></h3>
<p>PReLU 的全称为 Parametric Relu；PReLU 的全称为 Random ReLU。</p>
<p>这两个方法和 Leaky ReLU 类似，都是 ReLU 的变体。也都是为了解决 Dead ReLU 问题而提出来的。</p>
<p>Leaky ReLU 是在 <span class="arithmatex"><span class="MathJax_Preview">z&lt;0</span><script type="math/tex">z<0</script></span> 时，设置了一个较小的常数0.01作为斜率。由于这种常数值的斜率并不好，所以 PReLU 提出了可学习的斜率，RReLU 提出了随机的斜率，两者具体的公式如下。</p>
<p>PReLU 的公式如下，这里的 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 是可学习的：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\text{PReLU}(z) = \begin{cases}\alpha \cdot z &amp; \text{if } z \leqslant 0 \\z &amp; \text{if } z &gt; 0\end{cases}</div>
<script type="math/tex; mode=display">\text{PReLU}(z) = \begin{cases}\alpha \cdot z & \text{if } z \leqslant 0 \\z & \text{if } z > 0\end{cases}</script>
</div>
<p>RReLU 的公式如下，这里的 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 是从一个高斯分布中随机产生的，在训练过程中每次这个 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 都是不相同的；在推理时会将这个 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 设置为一个定值：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\text{RReLU}(z) = \begin{cases}\alpha \cdot z &amp; \text{if } z \leqslant 0 \\z &amp; \text{if } z &gt; 0\end{cases}</div>
<script type="math/tex; mode=display">\text{RReLU}(z) = \begin{cases}\alpha \cdot z & \text{if } z \leqslant 0 \\z & \text{if } z > 0\end{cases}</script>
</div>
<p>PReLU 和 RReLU 的图像如下所示：</p>
<div align=center><img src="/FLPUbCkI/assets/08.png" width=100% /></div>

<h2 id="6gelu">6、GELU<a class="headerlink" href="#6gelu" title="Permanent link">#</a></h2>
<blockquote>
<p>出自2016年的论文《Gaussian Error Linear Units (GELUs)》</p>
</blockquote>
<p>先描述一下 GELU 这个激活函数直觉上是基于一个什么思路设计出来的。然后再具体看其如何近似求解、如何代码实现。</p>
<h3 id="61-intuition">6.1 Intuition<a class="headerlink" href="#61-intuition" title="Permanent link">#</a></h3>
<p>先看一下 ReLU 激活函数是怎样做的，该函数中包含两种映射：一个是恒等映射（identity mapping），当输入值大于零时就是恒等映射；一个是置零映射（zero mapping），当输入值小于等于零时就是置零映射。</p>
<p>参考 ReLU 激活函数，设计另外一个包含恒等映射和置零映射的激活函数，并且参考 ReLU 函数来看，新激活函数应该有如下性质：</p>
<ol>
<li>在输入 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 满足某些条件时，为恒等映射；</li>
<li>在输入 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 满足另外一些条件时，为置零映射；</li>
<li>在输入 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 是一个较大的正值时，更希望为恒等映射；在输入 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 为一个较小的负值时，更希望是一个置零映射；</li>
</ol>
<p>以上就是想要新设计的激活函数的性质。</p>
<p>下面的图7和图8是标准正态分布的概率密度函数和累积分布函数的图像。接下来根据下图8中的累积分布函数设计一个新的函数。</p>
<p>符号定义：输入值用 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 表示，<span class="arithmatex"><span class="MathJax_Preview">\phi(\cdot)</span><script type="math/tex">\phi(\cdot)</script></span> 表示下图8中的正态分布的累积分布函数，<span class="arithmatex"><span class="MathJax_Preview">f(\cdot)</span><script type="math/tex">f(\cdot)</script></span> 表示新设计的函数。</p>
<p>设计的新函数：给定输入值 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>，函数 <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> 的输出值以 <span class="arithmatex"><span class="MathJax_Preview">\phi(x)</span><script type="math/tex">\phi(x)</script></span> 的概率采用恒等映射，以 <span class="arithmatex"><span class="MathJax_Preview">1-\phi(x)</span><script type="math/tex">1-\phi(x)</script></span> 的概率采用置零映射。也就是下述公式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
f(x) &amp;= x \cdot \phi(x) + 0 \cdot (1-\phi(x)) \\
&amp;= x \cdot \phi(x)
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
f(x) &= x \cdot \phi(x) + 0 \cdot (1-\phi(x)) \\
&= x \cdot \phi(x)
\end{split}\end{equation}</script>
</div>
<p>然后看一下，新设计的这个公式是否满足上述的激活函数性质。前两条是肯定满足的，主要看一下第3条性质：</p>
<ul>
<li>
<p>当输入 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 是一个较大的正值时，从图8中可以看出 <span class="arithmatex"><span class="MathJax_Preview">\phi(x)</span><script type="math/tex">\phi(x)</script></span> 的函数图像逐渐趋近于1，由于函数 <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> 的输出值以 <span class="arithmatex"><span class="MathJax_Preview">\phi(x)</span><script type="math/tex">\phi(x)</script></span> 的概率采用恒等映射，所以有接近于1的概率采用恒等映射；</p>
</li>
<li>
<p>当输入 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 是一个较小的负值时，<span class="arithmatex"><span class="MathJax_Preview">\phi(x)</span><script type="math/tex">\phi(x)</script></span> 趋近于0，由于函数 <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> 以 <span class="arithmatex"><span class="MathJax_Preview">1-\phi(x)</span><script type="math/tex">1-\phi(x)</script></span> 的概率采用置零映射，所以有接近于1的概率采用置零映射；</p>
</li>
</ul>
<p>可以看出新设计的这个函数是满足上述激活函数的性质的。</p>
<p>为了更直观描述设计该函数时的直觉，上述都是采用图8进行描述的，上述公式如果使用图7中的概率密度函数就是如下形式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
f(x) &amp;= x \cdot p(X&lt;x) + 0 \cdot (1-p(X&lt;x)) \\
&amp;= x \cdot p(X&lt;x)
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
f(x) &= x \cdot p(X<x) + 0 \cdot (1-p(X<x)) \\
&= x \cdot p(X<x)
\end{split}\end{equation}</script>
</div>
<p>其中 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 表示的实际的输入值，<span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 表示的是随机变量。到此就得到了 GELU 的常见形式了，公式如下所示：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}GELU = x \cdot p(X&lt;x) = x \cdot \phi(x)\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}GELU = x \cdot p(X<x) = x \cdot \phi(x)\end{split}\end{equation}</script>
</div>
<table>
<thead>
<tr>
<th align="center">图7: 标准正态分布概率密度函数</th>
<th align="center">图8: 标准正态分布累积分布函数</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><span class="arithmatex"><span class="MathJax_Preview">p(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}x^2}</span><script type="math/tex">p(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}x^2}</script></span></td>
<td align="center"><span class="arithmatex"><span class="MathJax_Preview">\phi(x) = \intop_{-\infty}^{x} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}t^2} dt</span><script type="math/tex">\phi(x) = \intop_{-\infty}^{x} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}t^2} dt</script></span></td>
</tr>
<tr>
<td align="center"><div align=center><img src="/FLPUbCkI/assets/09.png" width=100% /></div></td>
<td align="center"><div align=center><img src="/FLPUbCkI/assets/10.png" width=100% /></div></td>
</tr>
</tbody>
</table>
<blockquote>
<p>这里描述的设计 GELU 函数的直觉思路是非常简化的版本，只是为了易于理解。实际在设计这个函数时还需要考虑更多的因素，比如该函数的那几条性质和 ReLU 很像，已经有了 ReLU 为什么还要设计这个函数，这个函数在理论上是否能够解决 ReLU 的存在的 Dead ReLU 等问题；</p>
</blockquote>
<h3 id="62">6.2 函数以及导数图像<a class="headerlink" href="#62" title="Permanent link">#</a></h3>
<p>GELU 公式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}GELU = x \cdot \phi(x)\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}GELU = x \cdot \phi(x)\end{split}\end{equation}</script>
</div>
<p>使用该函数作为激活函数时，需要求解其导数。对其求导可得：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
\frac{d}{dx} GELU &amp;= \phi(x) + x \frac{d}{dx} \phi(x) \\
&amp;= \phi(x) + x \cdot p(X=x)
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
\frac{d}{dx} GELU &= \phi(x) + x \frac{d}{dx} \phi(x) \\
&= \phi(x) + x \cdot p(X=x)
\end{split}\end{equation}</script>
</div>
<p>其中 <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 是随机变量，<span class="arithmatex"><span class="MathJax_Preview">p(X=x)</span><script type="math/tex">p(X=x)</script></span> 是图7中的标准正态分布概率密度函数中，随机变量取值为 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 时的值。</p>
<p>GELU 函数及其导数的图像如下所示。可以看出其函数图像和 ReLU 非常相似，其导数图像也和 ReLU 的导数图像非常相似，不过该图像是连续的。</p>
<table>
<thead>
<tr>
<th align="center">图9: GELU函数</th>
<th align="center">图10: GELU导数</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><div align=center><img src="/FLPUbCkI/assets/12.png" width=100% /></div></td>
<td align="center"><div align=center><img src="/FLPUbCkI/assets/13.png" width=100% /></div></td>
</tr>
</tbody>
</table>
<p>GELU 激活函数的优缺点：</p>
<ul>
<li>从其函数图像可以看出，在负值区域，不再全为0，这解决了 Dead ReLU 问题；</li>
<li>GELU 函数是处处连续、光滑可导的；</li>
<li>（其他的不清楚了... ...）</li>
</ul>
<h3 id="63">6.3 加速计算<a class="headerlink" href="#63" title="Permanent link">#</a></h3>
<p>对于 GeLU 的加速计算有两种方法。</p>
<p>第一种方法是精确求解。有一个函数为 Gauss Error function (gef)，由于使用率非常高所以在常见的库（比如TensorFlow、PyTorch）中都有针对该函数的优化，该函数的公式如下。</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}\text{erf}(y) = \frac{2}{\sqrt{\pi}} \int_0^y e^{-t^2} dt\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}\text{erf}(y) = \frac{2}{\sqrt{\pi}} \int_0^y e^{-t^2} dt\end{split}\end{equation}</script>
</div>
<p>所以如果能够先求解出 <span class="arithmatex"><span class="MathJax_Preview">\text{erf}(\cdot)</span><script type="math/tex">\text{erf}(\cdot)</script></span>，再由该函数求解出 <span class="arithmatex"><span class="MathJax_Preview">\phi(x)</span><script type="math/tex">\phi(x)</script></span>，那么可以加快计算。下面省略具体的推导过程，直接给出通过 <span class="arithmatex"><span class="MathJax_Preview">\text{erf}(\cdot)</span><script type="math/tex">\text{erf}(\cdot)</script></span> 计算 <span class="arithmatex"><span class="MathJax_Preview">\phi(x)</span><script type="math/tex">\phi(x)</script></span> 的公式如下：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\phi(x) = \frac{1+\text{erf}(\frac{x}{\sqrt{2}})}{2}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\phi(x) = \frac{1+\text{erf}(\frac{x}{\sqrt{2}})}{2}\end{equation}</script>
</div>
<p>另一种方法是不精确求解，而是求解其近似值。为了加速计算，还可以使用近似计算的方式。GELU 的近似公式如下所示：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}GELU = 0.5 * x(1 + \tanh \big[ \sqrt{\frac{2}{\pi}} (x + 0.044715x^3) \big])\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}GELU = 0.5 * x(1 + \tanh \big[ \sqrt{\frac{2}{\pi}} (x + 0.044715x^3) \big])\end{split}\end{equation}</script>
</div>
<h2 id="7glu">7、GLU<a class="headerlink" href="#7glu" title="Permanent link">#</a></h2>
<p>详情请看文章 <a href="/1fb1JNJ6/">GLU 和 SwiGLU</a></p>
<h2 id="8swish">8、Swish<a class="headerlink" href="#8swish" title="Permanent link">#</a></h2>
<blockquote>
<p>出自2017年的论文《Searching for Activation Functions》</p>
</blockquote>
<p>该激活函数的公式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">f(x)=x \cdot \sigma (x)</div>
<script type="math/tex; mode=display">f(x)=x \cdot \sigma (x)</script>
</div>
<p>该激活函数的图像为：</p>
<div align=center><img src="/FLPUbCkI/assets/14.png" width=50% /></div>

<p>该激活函数的各种优缺点：</p>
<p>待补充... ...</p>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">#</a></h2>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/73214810">https://zhuanlan.zhihu.com/p/73214810</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/337902763">https://zhuanlan.zhihu.com/p/337902763</a></p>
</li>
<li>
<p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html">https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html</a></p>
</li>
<li>
<p><a href="https://alaaalatif.github.io/2019-04-11-gelu/">https://alaaalatif.github.io/2019-04-11-gelu/</a></p>
</li>
</ul></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
