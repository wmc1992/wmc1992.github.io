<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/006_LLM/004_%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84/02_QKV%E5%90%91%E9%87%8F%E5%90%84%E8%87%AA%E7%9A%84%E4%BD%9C%E7%94%A8/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Index - 算法工程师笔记</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../../css/extra.css" rel="stylesheet">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#self-attention-qkv" class="nav-link">分析 self-attention 中 QKV 三个向量各自的作用</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#1" class="nav-link">1、分析公式</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#11" class="nav-link">1.1 计算注意力权重矩阵</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#12-self-attention" class="nav-link">1.2 计算 self-attention 最终输出</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#2" class="nav-link">2、分析代码</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#reference" class="nav-link">Reference</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p align="right">[<a href="/X6carHHj_no_toc/">隐藏左侧目录栏</a>][<a href="/X6carHHj/">显示左侧目录栏</a>]</p>

<h1 id="self-attention-qkv">分析 self-attention 中 QKV 三个向量各自的作用<a class="headerlink" href="#self-attention-qkv" title="Permanent link">#</a></h1>
<p>首先 self-attention 的公式如下所示，这个都知道，但是 QKV 这三部分各自的作用一直不清晰，本文是对 QKV 各自具体的作用做一下分析。</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{Attention} = \text{softmax}(\frac{Q K^T}{\sqrt{d}}) \cdot V\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{Attention} = \text{softmax}(\frac{Q K^T}{\sqrt{d}}) \cdot V\end{equation}</script>
</div>
<h2 id="1">1、分析公式<a class="headerlink" href="#1" title="Permanent link">#</a></h2>
<p>为了后文的描述，先说明概念并定义符号。</p>
<p>在 attention 中，key 序列与 value 序列必然是同一个序列，而 query 序列却不一定与 key/value 序列是同一个序列。后文在描述时使用 "query序列" 和 "key/value序列" 这两种说法指代这两个序列。将 query 序列的长度定义为 <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>，将 key/value 序列的长度定义为 <span class="arithmatex"><span class="MathJax_Preview">L'</span><script type="math/tex">L'</script></span>，则 <span class="arithmatex"><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span>、<span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>、<span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> 这三个矩阵的维度为：<span class="arithmatex"><span class="MathJax_Preview">(L, d)</span><script type="math/tex">(L, d)</script></span>, <span class="arithmatex"><span class="MathJax_Preview">(L', d)</span><script type="math/tex">(L', d)</script></span>, <span class="arithmatex"><span class="MathJax_Preview">(L', d)</span><script type="math/tex">(L', d)</script></span>，其中 <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> 是每个 token 对应的向量的维度。</p>
<h3 id="11">1.1 计算注意力权重矩阵<a class="headerlink" href="#11" title="Permanent link">#</a></h3>
<p>把矩阵 <span class="arithmatex"><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> 中的每一行写成向量的形式，为：<span class="arithmatex"><span class="MathJax_Preview">Q = \begin{bmatrix}
   \vec{q_1} \\
   \vec{q_2} \\
   \vdots \\
   \vec{q_l} \\
   \vdots \\
   \vec{q_L}
\end{bmatrix}</span><script type="math/tex">Q = \begin{bmatrix}
   \vec{q_1} \\
   \vec{q_2} \\
   \vdots \\
   \vec{q_l} \\
   \vdots \\
   \vec{q_L}
\end{bmatrix}</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">K = \begin{bmatrix}
   \vec{k_1} \\
   \vec{k_2} \\
   \vdots \\
   \vec{k_{l'}} \\
   \vdots \\
   \vec{k_{L'}}
\end{bmatrix}</span><script type="math/tex">K = \begin{bmatrix}
   \vec{k_1} \\
   \vec{k_2} \\
   \vdots \\
   \vec{k_{l'}} \\
   \vdots \\
   \vec{k_{L'}}
\end{bmatrix}</script></span>，其中所有的元素 <span class="arithmatex"><span class="MathJax_Preview">\vec{q_l}</span><script type="math/tex">\vec{q_l}</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">\vec{k_{l'}}</span><script type="math/tex">\vec{k_{l'}}</script></span> 都是维度为 <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> 的向量。将这两个矩阵相乘得到：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
\frac{1}{\sqrt{d}} \cdot Q \cdot K^T  &amp;= \frac{1}{\sqrt{d}} \cdot \begin{bmatrix}
   \vec{q_1} \\
   \vec{q_2} \\
   \vdots \\
   \vec{q_l} \\
   \vdots \\
   \vec{q_L}
\end{bmatrix} \cdot [\vec{k_1^T}, \vec{k_2^T}, ..., \vec{k_{l'}^T}, ..., \vec{k_{L'}^T}] \\
&amp;= \frac{1}{\sqrt{d}} \cdot \begin{bmatrix}
   \vec{q_1}\vec{k_1^T} &amp; \vec{q_1}\vec{k_2^T} &amp; ... &amp; \vec{q_1}\vec{k_{l'}^T} &amp; ... &amp; \vec{q_1}\vec{k_{L'}^T} \\
   \vec{q_2}\vec{k_1^T} &amp; \vec{q_2}\vec{k_2^T} &amp; ... &amp; \vec{q_2}\vec{k_{l'}^T} &amp; ... &amp; \vec{q_2}\vec{k_{L'}^T} \\
   &amp; &amp; \vdots \\
   \vec{q_l}\vec{k_1^T} &amp; \vec{q_l}\vec{k_2^T} &amp; ... &amp; \vec{q_l}\vec{k_{l'}^T} &amp; ... &amp; \vec{q_l}\vec{k_{L'}^T} \\
   &amp; &amp; \vdots \\
   \vec{q_L}\vec{k_1^T} &amp; \vec{q_L}\vec{k_2^T} &amp; ... &amp; \vec{q_L}\vec{k_{l'}^T} &amp; ... &amp; \vec{q_L}\vec{k_{L'}^T}
\end{bmatrix}
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
\frac{1}{\sqrt{d}} \cdot Q \cdot K^T  &= \frac{1}{\sqrt{d}} \cdot \begin{bmatrix}
   \vec{q_1} \\
   \vec{q_2} \\
   \vdots \\
   \vec{q_l} \\
   \vdots \\
   \vec{q_L}
\end{bmatrix} \cdot [\vec{k_1^T}, \vec{k_2^T}, ..., \vec{k_{l'}^T}, ..., \vec{k_{L'}^T}] \\
&= \frac{1}{\sqrt{d}} \cdot \begin{bmatrix}
   \vec{q_1}\vec{k_1^T} & \vec{q_1}\vec{k_2^T} & ... & \vec{q_1}\vec{k_{l'}^T} & ... & \vec{q_1}\vec{k_{L'}^T} \\
   \vec{q_2}\vec{k_1^T} & \vec{q_2}\vec{k_2^T} & ... & \vec{q_2}\vec{k_{l'}^T} & ... & \vec{q_2}\vec{k_{L'}^T} \\
   & & \vdots \\
   \vec{q_l}\vec{k_1^T} & \vec{q_l}\vec{k_2^T} & ... & \vec{q_l}\vec{k_{l'}^T} & ... & \vec{q_l}\vec{k_{L'}^T} \\
   & & \vdots \\
   \vec{q_L}\vec{k_1^T} & \vec{q_L}\vec{k_2^T} & ... & \vec{q_L}\vec{k_{l'}^T} & ... & \vec{q_L}\vec{k_{L'}^T}
\end{bmatrix}
\end{split}\end{equation}</script>
</div>
<p>接下来就该对上述矩阵做 <span class="arithmatex"><span class="MathJax_Preview">\text{softmax}(\cdot)</span><script type="math/tex">\text{softmax}(\cdot)</script></span> 操作了，注意是对上述矩阵的每一行做 <span class="arithmatex"><span class="MathJax_Preview">\text{softmax}(\cdot)</span><script type="math/tex">\text{softmax}(\cdot)</script></span>，变形过程如下式，该矩阵就是注意力权重矩阵了：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
\text{att weight matrix} = \text{softmax}(\frac{1}{\sqrt{d}} \cdot Q \cdot K^T) &amp;= \begin{bmatrix}
   \text{softmax}(\begin{bmatrix}\vec{q_1}\vec{k_1^T} &amp; \vec{q_1}\vec{k_2^T} &amp; ... &amp; \vec{q_1}\vec{k_{l'}^T} &amp; ... &amp; \vec{q_1}\vec{k_{L'}^T}\end{bmatrix}) \\
   \text{softmax}(\begin{bmatrix}\vec{q_2}\vec{k_1^T} &amp; \vec{q_2}\vec{k_2^T} &amp; ... &amp; \vec{q_2}\vec{k_{l'}^T} &amp; ... &amp; \vec{q_2}\vec{k_{L'}^T}\end{bmatrix}) \\
   \vdots \\
   \text{softmax}(\begin{bmatrix}\vec{q_l}\vec{k_1^T} &amp; \vec{q_l}\vec{k_2^T} &amp; ... &amp; \vec{q_l}\vec{k_{l'}^T} &amp; ... &amp; \vec{q_l}\vec{k_{L'}^T}\end{bmatrix}) \\
   \vdots \\
   \text{softmax}(\begin{bmatrix}\vec{q_L}\vec{k_1^T} &amp; \vec{q_L}\vec{k_2^T} &amp; ... &amp; \vec{q_L}\vec{k_{l'}^T} &amp; ... &amp; \vec{q_L}\vec{k_{L'}^T}\end{bmatrix}) \\
\end{bmatrix} \\
&amp;= \begin{bmatrix}
   a_{11} &amp; a_{12} &amp; ... &amp; a_{1l'} &amp; ... &amp; a_{1L'} \\
   a_{21} &amp; a_{22} &amp; ... &amp; a_{2l'} &amp; ... &amp; a_{2L'} \\
   &amp; &amp; \vdots \\
   a_{l1} &amp; a_{l2} &amp; ... &amp; a_{ll'} &amp; ... &amp; a_{lL'} \\
   &amp; &amp; \vdots \\
   a_{L1} &amp; a_{L2} &amp; ... &amp; a_{Ll'} &amp; ... &amp; a_{LL'} \\
\end{bmatrix}
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
\text{att weight matrix} = \text{softmax}(\frac{1}{\sqrt{d}} \cdot Q \cdot K^T) &= \begin{bmatrix}
   \text{softmax}(\begin{bmatrix}\vec{q_1}\vec{k_1^T} & \vec{q_1}\vec{k_2^T} & ... & \vec{q_1}\vec{k_{l'}^T} & ... & \vec{q_1}\vec{k_{L'}^T}\end{bmatrix}) \\
   \text{softmax}(\begin{bmatrix}\vec{q_2}\vec{k_1^T} & \vec{q_2}\vec{k_2^T} & ... & \vec{q_2}\vec{k_{l'}^T} & ... & \vec{q_2}\vec{k_{L'}^T}\end{bmatrix}) \\
   \vdots \\
   \text{softmax}(\begin{bmatrix}\vec{q_l}\vec{k_1^T} & \vec{q_l}\vec{k_2^T} & ... & \vec{q_l}\vec{k_{l'}^T} & ... & \vec{q_l}\vec{k_{L'}^T}\end{bmatrix}) \\
   \vdots \\
   \text{softmax}(\begin{bmatrix}\vec{q_L}\vec{k_1^T} & \vec{q_L}\vec{k_2^T} & ... & \vec{q_L}\vec{k_{l'}^T} & ... & \vec{q_L}\vec{k_{L'}^T}\end{bmatrix}) \\
\end{bmatrix} \\
&= \begin{bmatrix}
   a_{11} & a_{12} & ... & a_{1l'} & ... & a_{1L'} \\
   a_{21} & a_{22} & ... & a_{2l'} & ... & a_{2L'} \\
   & & \vdots \\
   a_{l1} & a_{l2} & ... & a_{ll'} & ... & a_{lL'} \\
   & & \vdots \\
   a_{L1} & a_{L2} & ... & a_{Ll'} & ... & a_{LL'} \\
\end{bmatrix}
\end{split}\end{equation}</script>
</div>
<p>该矩阵的维度是 <span class="arithmatex"><span class="MathJax_Preview">(L, L')</span><script type="math/tex">(L, L')</script></span>，其中每一行对应的是 query 序列中某个 token 对 key/value 序列中所有 token 的注意力权重。把上述矩阵中的每一行对应的向量用一个符号 <span class="arithmatex"><span class="MathJax_Preview">\vec{a_l}</span><script type="math/tex">\vec{a_l}</script></span> 来表示，那么上述注意力权重矩阵就变为了 <span class="arithmatex"><span class="MathJax_Preview">\begin{bmatrix}
   \vec{a_1} \\
   \vec{a_2} \\
   \vdots \\
   \vec{a_l} \\
   \vdots \\
   \vec{a_L}
\end{bmatrix}</span><script type="math/tex">\begin{bmatrix}
   \vec{a_1} \\
   \vec{a_2} \\
   \vdots \\
   \vec{a_l} \\
   \vdots \\
   \vec{a_L}
\end{bmatrix}</script></span>，其中 <span class="arithmatex"><span class="MathJax_Preview">\vec{a_l}</span><script type="math/tex">\vec{a_l}</script></span> 是一个有 <span class="arithmatex"><span class="MathJax_Preview">L'</span><script type="math/tex">L'</script></span> 个元素的向量，该向量是经过softmax之后得到的，即其中的每个值都介于 0～1 之间，且该向量的所有元素之和为1。<span class="arithmatex"><span class="MathJax_Preview">\vec{a_l}</span><script type="math/tex">\vec{a_l}</script></span> 的含义为 query 序列中第 <span class="arithmatex"><span class="MathJax_Preview">l</span><script type="math/tex">l</script></span> 个 token 对 key/value 序列中所有 token 的注意力权重。</p>
<h3 id="12-self-attention">1.2 计算 self-attention 最终输出<a class="headerlink" href="#12-self-attention" title="Permanent link">#</a></h3>
<p>对于矩阵 <span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span>，和矩阵 <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> 一样，也是将每一行写成向量的形式，即：<span class="arithmatex"><span class="MathJax_Preview">V = \begin{bmatrix}
   \vec{v_1} \\
   \vec{v_2} \\
   \vdots \\
   \vec{v_{l'}} \\
   \vdots \\
   \vec{v_{L'}}
\end{bmatrix}</span><script type="math/tex">V = \begin{bmatrix}
   \vec{v_1} \\
   \vec{v_2} \\
   \vdots \\
   \vec{v_{l'}} \\
   \vdots \\
   \vec{v_{L'}}
\end{bmatrix}</script></span>，接下来就把上述注意力权重矩阵乘上矩阵 <span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span>，得到下式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
\text{att weight matrix} \cdot V &amp;= \begin{bmatrix}
   a_{11} &amp; a_{12} &amp; ... &amp; a_{1l'} &amp; ... &amp; a_{1L'} \\
   a_{21} &amp; a_{22} &amp; ... &amp; a_{2l'} &amp; ... &amp; a_{2L'} \\
   &amp; &amp; \vdots \\
   a_{l1} &amp; a_{l2} &amp; ... &amp; a_{ll'} &amp; ... &amp; a_{lL'} \\
   &amp; &amp; \vdots \\
   a_{L1} &amp; a_{L2} &amp; ... &amp; a_{Ll'} &amp; ... &amp; a_{LL'} \\
\end{bmatrix} \cdot \begin{bmatrix}
   \vec{v_1} \\
   \vec{v_2} \\
   \vdots \\
   \vec{v_{l'}} \\
   \vdots \\
   \vec{v_{L'}}
\end{bmatrix} \\
&amp;= \begin{bmatrix}
a_{11}\vec{v_1} + a_{12}\vec{v_2} + ... + a_{1l'}\vec{v_{l'}} + ... + a_{1L'}\vec{v_{L'}} \\
a_{21}\vec{v_1} + a_{22}\vec{v_2} + ... + a_{2l'}\vec{v_{l'}} + ... + a_{2L'}\vec{v_{L'}} \\
\vdots \\
a_{l1}\vec{v_1} + a_{l2}\vec{v_2} + ... + a_{ll'}\vec{v_{l'}} + ... + a_{lL'}\vec{v_{L'}} \\
\vdots \\
a_{L1}\vec{v_1} + a_{L2}\vec{v_2} + ... + a_{Ll'}\vec{v_{l'}} + ... + a_{LL'}\vec{v_{L'}}
\end{bmatrix} \\
&amp;= \begin{bmatrix}
\vec{\text{logits}_1} \\
\vec{\text{logits}_2} \\
\vdots \\
\vec{\text{logits}_l} \\
\vdots \\
\vec{\text{logits}_L}
\end{bmatrix}
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
\text{att weight matrix} \cdot V &= \begin{bmatrix}
   a_{11} & a_{12} & ... & a_{1l'} & ... & a_{1L'} \\
   a_{21} & a_{22} & ... & a_{2l'} & ... & a_{2L'} \\
   & & \vdots \\
   a_{l1} & a_{l2} & ... & a_{ll'} & ... & a_{lL'} \\
   & & \vdots \\
   a_{L1} & a_{L2} & ... & a_{Ll'} & ... & a_{LL'} \\
\end{bmatrix} \cdot \begin{bmatrix}
   \vec{v_1} \\
   \vec{v_2} \\
   \vdots \\
   \vec{v_{l'}} \\
   \vdots \\
   \vec{v_{L'}}
\end{bmatrix} \\
&= \begin{bmatrix}
a_{11}\vec{v_1} + a_{12}\vec{v_2} + ... + a_{1l'}\vec{v_{l'}} + ... + a_{1L'}\vec{v_{L'}} \\
a_{21}\vec{v_1} + a_{22}\vec{v_2} + ... + a_{2l'}\vec{v_{l'}} + ... + a_{2L'}\vec{v_{L'}} \\
\vdots \\
a_{l1}\vec{v_1} + a_{l2}\vec{v_2} + ... + a_{ll'}\vec{v_{l'}} + ... + a_{lL'}\vec{v_{L'}} \\
\vdots \\
a_{L1}\vec{v_1} + a_{L2}\vec{v_2} + ... + a_{Ll'}\vec{v_{l'}} + ... + a_{LL'}\vec{v_{L'}}
\end{bmatrix} \\
&= \begin{bmatrix}
\vec{\text{logits}_1} \\
\vec{\text{logits}_2} \\
\vdots \\
\vec{\text{logits}_l} \\
\vdots \\
\vec{\text{logits}_L}
\end{bmatrix}
\end{split}\end{equation}</script>
</div>
<p>先看上述公式（4）中间那个等号右侧的矩阵，该矩阵总共有 <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> 行，对应 <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> 个向量，每个向量是 key/value 序列中每个 token 对应的隐向量 <span class="arithmatex"><span class="MathJax_Preview">\vec{v}</span><script type="math/tex">\vec{v}</script></span> 按照注意力权重加权求和得到的。在最开始时就已经定义了 <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> 表示的是 query 序列的长度，所以在文本生成任务的背景下，公式（4）计算出的矩阵每行的向量的含义为：在 query 序列中，已知每个 t 时刻的 token，预测下一个 t+1 时刻的 token 时的 logits。也就是说，公式（4）中的 <span class="arithmatex"><span class="MathJax_Preview">\vec{\text{logits}_1}</span><script type="math/tex">\vec{\text{logits}_1}</script></span> 表示给定了 <span class="arithmatex"><span class="MathJax_Preview">t=1</span><script type="math/tex">t=1</script></span> 时刻的 token，预测下一个 <span class="arithmatex"><span class="MathJax_Preview">t=2</span><script type="math/tex">t=2</script></span> 时刻时计算出的 logits；<span class="arithmatex"><span class="MathJax_Preview">\vec{\text{logits}_l}</span><script type="math/tex">\vec{\text{logits}_l}</script></span> 表示给定了 <span class="arithmatex"><span class="MathJax_Preview">t=l</span><script type="math/tex">t=l</script></span> 时刻的 token，预测下一个 <span class="arithmatex"><span class="MathJax_Preview">t=l+1</span><script type="math/tex">t=l+1</script></span> 时刻时计算出的 logits。</p>
<p>所以可见，常规的 self-attention 的公式会把整个 query 序列中，给定每个 token，预测下一个 token 的所有 logits 都计算出来。对于 clm 任务，这在训练时是没有问题的，因为训练时是希望并行的，一次性把整个序列的 loss 全都计算出来。但是在推理时，我们仅想知道最后一个 token 的下一个 token 是什么，前面的并不想知道，所以除了最后一个 token 对应的 logits 需要计算以外其他的都不需要计算，常规的 self-attention 是有些计算资源的浪费的，不过使用了 <a href="/Ao5PjBRP/">KVCache</a> 技术之后这部分浪费基本就被优化掉了。</p>
<h2 id="2">2、分析代码<a class="headerlink" href="#2" title="Permanent link">#</a></h2>
<p>依然还是使用 GPT2 的代码做分析，如果对原始的 GPT2 模型结构和代码不了解，请参考 <a href="/wiyOMRd9/">GPT2 模型结构和实现代码</a>。</p>
<p>与分析公式中相同，key 序列与 value 序列必然是同一个序列，而 query 序列却不一定与 key/value 序列是同一个序列。注释里描述时使用 "query序列" 和 "key/value序列" 这两种说法指代这两个序列。在下述代码的注释中使用 <code>q_seq_len</code> 用来说明 query 序列的长度，使用 <code>kv_seq_len</code> 用来说明 key/value 序列的长度。</p>
<p>对代码的说明都放在注释中了，在 <code>forward()</code> 函数中注释比代码还要长。</p>
<pre><code class="language-python">class GTP2Attention(nn.Module):
    &quot;&quot;&quot; 该类为一个完整的 multi-head attention 结构 &quot;&quot;&quot;

    def __init__(self, config):
        super().__init__()

        self.embed_dim = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.embed_dim // self.num_heads
        self.split_size = self.embed_dim

        self.c_attn = nn.Linear(self.embed_dim, 3 * self.embed_dim)
        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)

        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        self.resid_dropout = nn.Dropout(config.resid_pdrop)

    def forward(self, hidden_states, attention_mask=None):
        # 将 W_q, W_k, W_v 三个矩阵合成一次矩阵乘法运算，乘完之后再分割开来
        # query: [bs, q_seq_len, embed_dim]
        # key:   [bs, kv_seq_len, embed_dim]
        # value: [bs, kv_seq_len, embed_dim]
        query, key, value = self.c_attn(hidden_states).split(self.embed_dim, dim=2)

        # ========================================================================
        # 经过下面这几行代码的变形之后，query, key, value 的维度如下：
        #   query: [bs, num_heads, q_seq_len, head_dim]
        #   key  : [bs, num_heads, kv_seq_len, head_dim]
        #   value: [bs, num_heads, kv_seq_len, head_dim]
        #
        # 在后面的分析中基本都会忽略掉 batch_size 以及多头部分，仅分析后两个维度
        # ========================================================================
        q_new_size = query.size()[:-1] + [self.num_heads, self.head_dim]
        query = query.view(q_new_size)
        kv_new_size = key.size()[:-1] + [self.num_heads, self.head_dim]
        key, value = key.view(kv_new_size), value.view(kv_new_size)
        query, key, value = query.permute(0, 2, 1, 3), key.permute(0, 2, 1, 3), value.permute(0, 2, 1, 3)

        # ========================================================================
        # 下面这行代码对应着上文的公式（2），当然这里没有除以根号d；
        # 这行代码得到的注意力权重矩阵 att_weights 的维度为: [bs, num_heads, q_seq_len, kv_seq_len]
        # ========================================================================
        att_weights = torch.matmul(query, key.transpose(-1, -2))

        # ========================================================================
        # 对刚才得到的注意力矩阵除上一个根号d_k，然后再做softmax；
        # 注意这里做 softmax 时是对最后一个维度做的，与上面的公式（3）是一致的；
        # 经过 softmax 之后得到的就是最终的注意力权重矩阵了，维度还是: [bs, num_heads, q_seq_len, kv_seq_len]
        # 忽略前两个维度之后，(q_seq_len, kv_seq_len) 表示的就是 query 序列中的 token 对 key/value 序列中的每个 token 的注意力权重
        # ========================================================================
        att_weights = att_weights / torch.full([], value.size()[-1] ** 0.5)
        if attention_mask is not None:
            att_weights = att_weights + attention_mask
        att_weights = nn.functional.softmax(att_weights, dim=-1)
        att_weights = self.attn_dropout(att_weights)

        # ========================================================================
        # 下述矩阵乘法的维度变换为: 
        #   [bs, num_heads, q_seq_len, head_dim] = [bs, num_heads, q_seq_len, kv_seq_len] * [bs, num_heads, kv_seq_len, head_dim]
        # 
        # 忽略前两个维度，只看后两个维度，即: [q_seq_len, head_dim] = [q_seq_len, kv_seq_len] * [kv_seq_len, head_dim]
        # 
        # 这里的矩阵乘法的含义为：将 key/value 序列的 value 矩阵中每个隐层的向量按照 `query 序列中的每个 token 对 key/value 序列中每个 token 的关注权重` 进行加权求和。
        #
        # 这里的  `query 序列中的每个 token 对 key/value 序列中每个 token 的关注权重` 就是上面解释的注意力权重矩阵 att_weights 的含义。
        #
        # 最终得到的矩阵 att_output 的维度为: [bs, num_heads, q_seq_len, head_dim]，忽略前两个维度，只看 (q_seq_len, head_dim)，这个矩阵的其实就是：
        # 
        #    对 query 序列中的每个字符生成下一个字符时的 logits 向量
        # ========================================================================
        att_output = torch.matmul(att_weights, value)

        # 注意：这里不能够直接将 att_output 由 [bs, num_heads, seq_len, head_dim] 变为 [bs, seq_len, embed_dim]
        # [bs, num_heads, seq_len, head_dim] -&gt; [bs, seq_len, num_heads, head_dim] -&gt; [bs, seq_len, embed_dim]
        att_output = att_output.permute(0, 2, 1, 3)
        att_output = att_output.view(att_output.size()[:-2] + [self.num_heads * self.head_dim, ])

        att_output = self.c_proj(att_output)
        att_output = self.resid_dropout(att_output)

        return att_output
</code></pre>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">#</a></h2>
<ul>
<li><a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></li>
</ul></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="../../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
