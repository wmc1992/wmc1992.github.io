<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/005_%E5%B7%A5%E5%85%B7%E6%A1%86%E6%9E%B6/001_huggingface/exp_with_accelerate_and_deepspeed/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Index - 算法工程师笔记</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../../css/extra.css" rel="stylesheet">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-12" role="main">

<p align="right">[<a href="/OfubLsO7_no_toc/">隐藏左侧目录栏</a>][<a href="/OfubLsO7/">显示左侧目录栏</a>]</p>

<h1 id="experiment-with-transformers">Experiment With Transformers<a class="headerlink" href="#experiment-with-transformers" title="Permanent link">#</a></h1>
<h2 id="1">1、全量参数微调<a class="headerlink" href="#1" title="Permanent link">#</a></h2>
<p>先设计一下要跑哪几个实验：</p>
<ul>
<li>使用 accelerate 库在单卡上启动  transformers 库中提供的训练脚本；</li>
<li>使用 accelerate 库在双卡（数据并行）上启动  transformers 库中提供的训练脚本；</li>
<li>使用 deepspeed 库在单卡（ZeRO3）上启动  transformers 库中提供的训练脚本；</li>
<li>使用 deepspeed 库在双卡（ZeRO3）上启动  transformers 库中提供的训练脚本；</li>
</ul>
<h3 id="11-accelerate">1.1 Accelerate 单卡<a class="headerlink" href="#11-accelerate" title="Permanent link">#</a></h3>
<p>accelerate 的配置：</p>
<pre><code class="language-yaml">compute_environment: LOCAL_MACHINE
distributed_type: MULTI_GPU
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0
main_training_function: main
mixed_precision: fp16
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
</code></pre>
<p>启动命令：</p>
<pre><code>accelerate launch \
  --config_file ./myconfig/default_accelerate_config.yaml \
  examples/pytorch/language-modeling/run_clm.py \
  --model_name_or_path gpt2-xl \
  --train_file ./data/data.json \
  --output_dir ./outputs \
  --overwrite_output_dir \
  --do_train \
  --do_eval \
  --fp16 \
  --per_device_train_batch_size 1 \
  --per_device_eval_batch_size 1
</code></pre>
<p>训练日志：</p>
<pre><code>[INFO|trainer.py:1779] 2023-05-22 00:42:36,114 &gt;&gt; ***** Running training *****
[INFO|trainer.py:1780] 2023-05-22 00:42:36,114 &gt;&gt;   Num examples = 38,008
[INFO|trainer.py:1781] 2023-05-22 00:42:36,114 &gt;&gt;   Num Epochs = 3
[INFO|trainer.py:1782] 2023-05-22 00:42:36,114 &gt;&gt;   Instantaneous batch size per device = 1
[INFO|trainer.py:1783] 2023-05-22 00:42:36,114 &gt;&gt;   Total train batch size (w. parallel, distributed &amp; accumulation) = 1
[INFO|trainer.py:1784] 2023-05-22 00:42:36,114 &gt;&gt;   Gradient Accumulation steps = 1
[INFO|trainer.py:1785] 2023-05-22 00:42:36,114 &gt;&gt;   Total optimization steps = 114,024
[INFO|trainer.py:1786] 2023-05-22 00:42:36,115 &gt;&gt;   Number of trainable parameters = 1,557,611,200
  1%|▉                                                                  | 650/114024 [10:06&lt;26:31:56,  1.19it/s]
</code></pre>
<p>显存使用情况：</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A40          Off  | 00000000:31:00.0 Off |                    0 |
|  0%   66C    P0   262W / 300W |  44403MiB / 46068MiB |     99%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A40          Off  | 00000000:4B:00.0 Off |                    0 |
|  0%   32C    P8    15W / 300W |      7MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
</code></pre>
<h3 id="12-accelerate">1.2 Accelerate 双卡<a class="headerlink" href="#12-accelerate" title="Permanent link">#</a></h3>
<p>accelerate 的配置：</p>
<pre><code class="language-yaml">compute_environment: LOCAL_MACHINE
distributed_type: MULTI_GPU
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0
main_training_function: main
mixed_precision: fp16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
</code></pre>
<p>启动命令：</p>
<pre><code>accelerate launch \
  --config_file ./myconfig/default_accelerate_config.yaml \
  examples/pytorch/language-modeling/run_clm.py \
  --model_name_or_path gpt2-xl \
  --train_file ./data/data.json \
  --output_dir ./outputs \
  --overwrite_output_dir \
  --do_train \
  --do_eval \
  --fp16 \
  --per_device_train_batch_size 1 \
  --per_device_eval_batch_size 1
</code></pre>
<p>训练日志：</p>
<pre><code>[INFO|trainer.py:1779] 2023-05-22 00:56:37,438 &gt;&gt; ***** Running training *****
[INFO|trainer.py:1780] 2023-05-22 00:56:37,438 &gt;&gt;   Num examples = 38,008
[INFO|trainer.py:1781] 2023-05-22 00:56:37,438 &gt;&gt;   Num Epochs = 3
[INFO|trainer.py:1782] 2023-05-22 00:56:37,438 &gt;&gt;   Instantaneous batch size per device = 1
[INFO|trainer.py:1783] 2023-05-22 00:56:37,438 &gt;&gt;   Total train batch size (w. parallel, distributed &amp; accumulation) = 2
[INFO|trainer.py:1784] 2023-05-22 00:56:37,438 &gt;&gt;   Gradient Accumulation steps = 1
[INFO|trainer.py:1785] 2023-05-22 00:56:37,438 &gt;&gt;   Total optimization steps = 57,012
[INFO|trainer.py:1786] 2023-05-22 00:56:37,440 &gt;&gt;   Number of trainable parameters = 1,557,611,200
  1%|█▉                                                                 | 673/57012 [10:57&lt;14:01:08,  1.12it/s]
</code></pre>
<p>显存使用情况：</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A40          Off  | 00000000:31:00.0 Off |                    0 |
|  0%   66C    P0   257W / 300W |  44389MiB / 46068MiB |     99%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A40          Off  | 00000000:4B:00.0 Off |                    0 |
|  0%   61C    P0   246W / 300W |  44385MiB / 46068MiB |     99%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
</code></pre>
<h3 id="13-deepspeed">1.3 DeepSpeed 单卡<a class="headerlink" href="#13-deepspeed" title="Permanent link">#</a></h3>
<p>deepspeed 的配置：</p>
<pre><code class="language-json">{
    &quot;fp16&quot;: {
        &quot;enabled&quot;: &quot;auto&quot;,
        &quot;loss_scale&quot;: 0,
        &quot;loss_scale_window&quot;: 1000,
        &quot;initial_scale_power&quot;: 16,
        &quot;hysteresis&quot;: 2,
        &quot;min_loss_scale&quot;: 1
    },

    &quot;optimizer&quot;: {
        &quot;type&quot;: &quot;AdamW&quot;,
        &quot;params&quot;: {
            &quot;lr&quot;: &quot;auto&quot;,
            &quot;betas&quot;: &quot;auto&quot;,
            &quot;eps&quot;: &quot;auto&quot;,
            &quot;weight_decay&quot;: &quot;auto&quot;
        }
    },

    &quot;scheduler&quot;: {
        &quot;type&quot;: &quot;WarmupLR&quot;,
        &quot;params&quot;: {
            &quot;warmup_min_lr&quot;: &quot;auto&quot;,
            &quot;warmup_max_lr&quot;: &quot;auto&quot;,
            &quot;warmup_num_steps&quot;: &quot;auto&quot;
        }
    },

    &quot;zero_optimization&quot;: {
        &quot;stage&quot;: 3,
        &quot;overlap_comm&quot;: true,
        &quot;contiguous_gradients&quot;: true,
        &quot;sub_group_size&quot;: 1e9,
        &quot;reduce_bucket_size&quot;: &quot;auto&quot;,
        &quot;stage3_prefetch_bucket_size&quot;: &quot;auto&quot;,
        &quot;stage3_param_persistence_threshold&quot;: &quot;auto&quot;,
        &quot;stage3_max_live_parameters&quot;: 1e9,
        &quot;stage3_max_reuse_distance&quot;: 1e9,
        &quot;stage3_gather_16bit_weights_on_model_save&quot;: true
    },

    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,
    &quot;gradient_clipping&quot;: &quot;auto&quot;,
    &quot;train_batch_size&quot;: &quot;auto&quot;,
    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;
}
</code></pre>
<p>启动命令：</p>
<pre><code>deepspeed --num_gpus=1 \
  examples/pytorch/language-modeling/run_clm.py \
  --deepspeed ./myconfig/ds_config.json \
  --model_name_or_path gpt2-xl \
  --train_file ./data/data.json \
  --output_dir ./outputs \
  --overwrite_output_dir \
  --do_train \
  --do_eval \
  --fp16 \
  --per_device_train_batch_size 1 \
  --per_device_eval_batch_size 1
</code></pre>
<p>训练日志：</p>
<pre><code>[INFO|trainer.py:1779] 2023-05-22 00:26:45,120 &gt;&gt; ***** Running training *****
[INFO|trainer.py:1780] 2023-05-22 00:26:45,120 &gt;&gt;   Num examples = 38,008
[INFO|trainer.py:1781] 2023-05-22 00:26:45,120 &gt;&gt;   Num Epochs = 3
[INFO|trainer.py:1782] 2023-05-22 00:26:45,120 &gt;&gt;   Instantaneous batch size per device = 1
[INFO|trainer.py:1783] 2023-05-22 00:26:45,120 &gt;&gt;   Total train batch size (w. parallel, distributed &amp; accumulation) = 1
[INFO|trainer.py:1784] 2023-05-22 00:26:45,120 &gt;&gt;   Gradient Accumulation steps = 1
[INFO|trainer.py:1785] 2023-05-22 00:26:45,120 &gt;&gt;   Total optimization steps = 114,024
[INFO|trainer.py:1786] 2023-05-22 00:26:45,121 &gt;&gt;   Number of trainable parameters = 1,557,611,200
  1%|▊                                                                  | 577/114024 [11:33&lt;38:41:41,  1.23s/it]
</code></pre>
<p>显存使用情况：</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A40          Off  | 00000000:31:00.0 Off |                    0 |
|  0%   56C    P0   164W / 300W |  40397MiB / 46068MiB |     52%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A40          Off  | 00000000:4B:00.0 Off |                    0 |
|  0%   35C    P0    75W / 300W |      4MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
</code></pre>
<h3 id="14-deepspeed">1.4 DeepSpeed 双卡<a class="headerlink" href="#14-deepspeed" title="Permanent link">#</a></h3>
<blockquote>
<p>优化器分片+梯度分片+参数分片；</p>
</blockquote>
<p>deepspeed 的配置：</p>
<pre><code class="language-json">{
    &quot;fp16&quot;: {
        &quot;enabled&quot;: &quot;auto&quot;,
        &quot;loss_scale&quot;: 0,
        &quot;loss_scale_window&quot;: 1000,
        &quot;initial_scale_power&quot;: 16,
        &quot;hysteresis&quot;: 2,
        &quot;min_loss_scale&quot;: 1
    },

    &quot;optimizer&quot;: {
        &quot;type&quot;: &quot;AdamW&quot;,
        &quot;params&quot;: {
            &quot;lr&quot;: &quot;auto&quot;,
            &quot;betas&quot;: &quot;auto&quot;,
            &quot;eps&quot;: &quot;auto&quot;,
            &quot;weight_decay&quot;: &quot;auto&quot;
        }
    },

    &quot;scheduler&quot;: {
        &quot;type&quot;: &quot;WarmupLR&quot;,
        &quot;params&quot;: {
            &quot;warmup_min_lr&quot;: &quot;auto&quot;,
            &quot;warmup_max_lr&quot;: &quot;auto&quot;,
            &quot;warmup_num_steps&quot;: &quot;auto&quot;
        }
    },

    &quot;zero_optimization&quot;: {
        &quot;stage&quot;: 3,
        &quot;overlap_comm&quot;: true,
        &quot;contiguous_gradients&quot;: true,
        &quot;sub_group_size&quot;: 1e9,
        &quot;reduce_bucket_size&quot;: &quot;auto&quot;,
        &quot;stage3_prefetch_bucket_size&quot;: &quot;auto&quot;,
        &quot;stage3_param_persistence_threshold&quot;: &quot;auto&quot;,
        &quot;stage3_max_live_parameters&quot;: 1e9,
        &quot;stage3_max_reuse_distance&quot;: 1e9,
        &quot;stage3_gather_16bit_weights_on_model_save&quot;: true
    },

    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,
    &quot;gradient_clipping&quot;: &quot;auto&quot;,
    &quot;train_batch_size&quot;: &quot;auto&quot;,
    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;
}
</code></pre>
<p>启动命令：</p>
<pre><code>deepspeed --num_gpus=2 \
  examples/pytorch/language-modeling/run_clm.py \
  --deepspeed ./myconfig/ds_config.json \
  --model_name_or_path gpt2-xl \
  --train_file ./data/data.json \
  --output_dir ./outputs \
  --overwrite_output_dir \
  --do_train \
  --do_eval \
  --fp16 \
  --per_device_train_batch_size 2 \
  --per_device_eval_batch_size 2
</code></pre>
<p>训练日志：</p>
<pre><code>[INFO|trainer.py:1779] 2023-05-22 00:13:55,728 &gt;&gt; ***** Running training *****
[INFO|trainer.py:1780] 2023-05-22 00:13:55,728 &gt;&gt;   Num examples = 38,008
[INFO|trainer.py:1781] 2023-05-22 00:13:55,728 &gt;&gt;   Num Epochs = 3
[INFO|trainer.py:1782] 2023-05-22 00:13:55,728 &gt;&gt;   Instantaneous batch size per device = 2
[INFO|trainer.py:1783] 2023-05-22 00:13:55,728 &gt;&gt;   Total train batch size (w. parallel, distributed &amp; accumulation) = 4
[INFO|trainer.py:1784] 2023-05-22 00:13:55,728 &gt;&gt;   Gradient Accumulation steps = 1
[INFO|trainer.py:1785] 2023-05-22 00:13:55,728 &gt;&gt;   Total optimization steps = 28,506
[INFO|trainer.py:1786] 2023-05-22 00:13:55,730 &gt;&gt;   Number of trainable parameters = 1,557,611,200
 2%|██▊                                                                 | 500/28506 [10:14&lt;9:40:02,  1.24s/it]
</code></pre>
<p>显存使用情况：</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A40          Off  | 00000000:31:00.0 Off |                    0 |
|  0%   62C    P0   216W / 300W |  41777MiB / 46068MiB |     75%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A40          Off  | 00000000:4B:00.0 Off |                    0 |
|  0%   58C    P0   219W / 300W |  41773MiB / 46068MiB |     96%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
</code></pre></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="../../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
