<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/006_LLM/999_%E5%85%B6%E4%BB%96/Accelerate%E5%AF%B9%E6%A8%A1%E5%9E%8B%E5%88%86%E6%AD%A5%E5%8A%A0%E8%BD%BD/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Accelerate - 算法工程师笔记</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../../css/extra.css" rel="stylesheet">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-12" role="main">

<h1 id="accelerate">Accelerate<a class="headerlink" href="#accelerate" title="Permanent link">#</a></h1>
<h2 id="_1">官方资料<a class="headerlink" href="#_1" title="Permanent link">#</a></h2>
<p>quickstart: <a href="https://huggingface.co/docs/accelerate/usage_guides/big_modeling">https://huggingface.co/docs/accelerate/usage_guides/big_modeling</a></p>
<p>api reference: <a href="https://huggingface.co/docs/accelerate/package_reference/big_modeling">https://huggingface.co/docs/accelerate/package_reference/big_modeling</a></p>
<h2 id="_2">样例代码跑通<a class="headerlink" href="#_2" title="Permanent link">#</a></h2>
<pre><code class="language-python">from accelerate import init_empty_weights, load_checkpoint_and_dispatch
from huggingface_hub import hf_hub_download
from transformers import AutoConfig, AutoModelForCausalLM

# Download the Weights
checkpoint = &quot;EleutherAI/gpt-j-6B&quot;
weights_location = hf_hub_download(checkpoint, &quot;pytorch_model.bin&quot;)

# 一、先创建一个空模型
config = AutoConfig.from_pretrained(checkpoint)
with init_empty_weights():
    model = AutoModelForCausalLM.from_config(config)

# 二、然后是加载权重。加载权重有两种方式：自动模式和手动指定模式。一般都使用自动模式即可。

# 采用如下这种写法的话，参数 no_split_module_classes 表示哪些block作为最小模块，不能够再分；至于哪
# 些模块加载到哪个设备上，是accelerate自动决定的；
model = load_checkpoint_and_dispatch(
    model, weights_location, device_map=&quot;auto&quot;, no_split_module_classes=[&quot;GPTJBlock&quot;]
)

# 采用如下这种写法的话，参数 device_map 就是完全自己定义哪些模块加载到哪个设备上
# my_device_map = None
# model = load_checkpoint_and_dispatch(model, weights_location, device_map=my_device_map)
</code></pre>
<p>上述代码直接跑就能跑通，唯一需要注意的是函数 <code>load_checkpoint_and_dispatch()</code> 的第二个参数是指定checkpoint的路径，这个路径是指定到具体的模型文件路径，而不是文件夹的路径。</p>
<h2 id="bloom">在Bloom上试验<a class="headerlink" href="#bloom" title="Permanent link">#</a></h2>
<p>试验使用的代码如下：</p>
<pre><code class="language-python">import torch

from accelerate import init_empty_weights, load_checkpoint_and_dispatch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer
from transformers.models.bloom import BloomForCausalLM

checkpoint = &quot;bigscience/bloom-560m&quot;
weights_location = hf_hub_download(checkpoint, &quot;pytorch_model.bin&quot;)

tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# 加载空模型
with init_empty_weights():
    model = BloomForCausalLM.from_pretrained(checkpoint)

# 加载权重
model = load_checkpoint_and_dispatch(
    model, weights_location, device_map=&quot;auto&quot;
)

# 推理
prompt = &quot;I believe the meaning of life is&quot;
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
result = tokenizer.decode(torch.argmax(outputs.logits, dim=-1)[0])
print(prompt, result)
</code></pre>
<p>上述代码执行时会出现如下报错：</p>
<pre><code>AttributeError: 'BloomForCausalLM' object has no attribute 'word_embeddings'
</code></pre>
<p>报错的原因是：</p>
<p>在上述代码中，初始化模型结构时用的类是 <code>BloomForCausalLM</code>，这个类是用来做文本生成的。它有两个属性，一个是<code>BloomMode</code>，一个是最后一层的分类器。而huggingface hub上放的 <code>bigscience/bloom-560m</code> 的权重是只有<code>BloomMode</code>的权重，没有最后一层分类器的权重的。同时由于类<code>BloomForCausalLM</code>中每层的命名是在<code>BloomMode</code>的外面又包了一层的，所以如果使用<code>strict=True</code>的方式加载模型，就都对应不上了。而函数<code>load_checkpoint_and_dispatch()</code>目前只有<code>strict=True</code>这种形式。</p>
<p>另外，即然模型Bloom系统都没有放出最后一层分类器的权重，那么我们就不应该直接使用这个模型做文本生成任务，因为最后一层的权重是随机初始化的。</p>
<h2 id="llama">在LLaMA上试验<a class="headerlink" href="#llama" title="Permanent link">#</a></h2>
<p>因为这个模型还没有添加到huggingface的transformers中，这个使用的代码是在LLaMA的官方代码的基础上做的修改。官方代码链接为：<a href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a>。</p>
<p>在官方代码 <code>example.py</code> 中做如下修改。仅做如下修改就可以跑通，然后其他的代码有些不需要的也可以注释掉。</p>
<p>原代码：</p>
<pre><code class="language-python">tokenizer = Tokenizer(model_path=tokenizer_path)
model_args.vocab_size = tokenizer.n_words
model = Transformer(model_args)
torch.set_default_tensor_type(torch.FloatTensor)
model.load_state_dict(checkpoint, strict=False)
</code></pre>
<p>修改后的代码：</p>
<pre><code class="language-python">from accelerate import init_empty_weights, load_checkpoint_and_dispatch

tokenizer = Tokenizer(model_path=tokenizer_path)
model_args.vocab_size = tokenizer.n_words
with init_empty_weights():
    model = Transformer(model_args)
torch.set_default_tensor_type(torch.FloatTensor)
model = load_checkpoint_and_dispatch(model, ckpt_path, device_map=&quot;auto&quot;)
</code></pre>
<p>上述代码执行时会出现如下报错：</p>
<pre><code>AttributeError: 'Attention' object has no attribute 'inner_attention'
</code></pre>
<p>这个原因也是类似的：就是代码中定义的模型结构与checkpoint中的不一致导致的。在checkpoint有一些是仅在训练时需要，而推理时不需要的权重，手动修改checkpoint，将这部分权重丢弃掉就可以了。</p>
<h2 id="no_split_module_classes">参数 <code>no_split_module_classes</code><a class="headerlink" href="#no_split_module_classes" title="Permanent link">#</a></h2>
<p>在使用函数 <code>load_checkpoint_and_dispatch()</code> 时有一个比较重要的输入参数是 <code>no_split_module_classes</code>，这个参数的作用是将传入的类名对应的模型加载到同一个设备上。下面举个例子说明。</p>
<p>LLaMA的官方代码中模型结构相关的定义是下面这样的：</p>
<pre><code class="language-python">
class TransformerBlock(nn.Module):
    def __init__(self, layer_id: int, args: ModelArgs):
        ...

class Transformer(nn.Module):
    def __init__(self, params: ModelArgs):
        ...
        self.layers = torch.nn.ModuleList()
        for layer_id in range(params.n_layers):
            self.layers.append(TransformerBlock(layer_id, params))
        ...
</code></pre>
<p>这里的 <code>TransformerBlock</code> 表示一层Transformer，我们现在想要让每层的transformer都加载到同一个设备上，那么使用函数 <code>load_checkpoint_and_dispatch()</code> 时就应该如下所示：</p>
<pre><code class="language-python">model = load_checkpoint_and_dispatch(model, ckpt_path, device_map=&quot;auto&quot;, no_split_module_classes=[&quot;TransformerBlock&quot;])
</code></pre>
<h2 id="_3">性能说明<a class="headerlink" href="#_3" title="Permanent link">#</a></h2>
<p>使用6张2080Ti做推理，模型是LLaMA的7B的模型，生成的文本最大长度设置为512，实际测试每条样本推理耗时为14s。</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="../../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
