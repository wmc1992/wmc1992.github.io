<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/003_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/008_%E5%8F%82%E6%95%B0%E9%87%8F%E4%BC%B0%E8%AE%A1%E4%B8%8E%E6%98%BE%E5%AD%98%E4%BC%B0%E8%AE%A1/">
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>参数量估计与显存估计 - 算法工程师笔记</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../css/extra.css" rel="stylesheet">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#_1" class="nav-link">参数量估计与显存估计</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#1" class="nav-link">1、根据模型结构估计参数量</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#2" class="nav-link">2、根据参数量估计模型大小</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#3" class="nav-link">3、根据参数量估计训练时显存大小</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#31-model-states" class="nav-link">3.1 Model States</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#32-residual-states" class="nav-link">3.2 Residual States</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#reference" class="nav-link">Reference</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="_1">参数量估计与显存估计<a class="headerlink" href="#_1" title="Permanent link">#</a></h1>
<h2 id="1">1、根据模型结构估计参数量<a class="headerlink" href="#1" title="Permanent link">#</a></h2>
<p>以 bert-base 为例估计模型的参数量。</p>
<p>bert-base 的模型结构为：vocab 总共有 21128 个，隐藏层维度为 768，12 层 transformer，multi-head attention 中是 12 个 head，每个 head 的隐层维度是 64。官方论文中总参数量为 110M。</p>
<p>估计 Bert 模型的参数量：</p>
<ul>
<li>
<p><strong>embedding 层</strong>：总共 21128 个 vocab，每个向量的维度是 768，总参数量为 21128 * 768 = 16226304</p>
</li>
<li>
<p><strong>multi-head attention 层</strong>：这一层带有参数的有三部分，分别为 QKV 的权重矩阵、dense 层、Layer Norm层，分别计算</p>
<ul>
<li>QKV 的权重矩阵：每个权重矩阵的输入向量的维度为 768，有 12 个 head，每个 head 的隐层维度是 64，则参数量为 768 * 64 * 12 = 589824，三个权重矩阵的参数量就是再乘以3，即 768 * 64 * 12 * 3 = 1769472</li>
<li>dense 层：这个比较简单，直接就是 768 * 768 = 589824</li>
<li>Layer Norm层：这一层里面不是矩阵乘法，而是向量的相应位置的元素相乘，所以 <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> 的参数量是 768 + 768 = 1536</li>
</ul>
</li>
<li>
<p><strong>FFN 层</strong>：这一层包含两个 dense 层，这两个 dense 层先将维度由 768 升到 3072，再由 3072 降到 768，然后接一个 Layer Norm 层，所以参数量为 (768 * 3072) + (3072 * 768) + (768 + 768) = 4720128</p>
</li>
<li>
<p><strong>总共12个 transformer 层</strong>：这个就是 multi-head attention 层的参数量加上 FFN 层的参数量，然后乘以 12 即可，每层 transformer 的参数量 1769472 + 589824 + 1536 + 4720128 = 7080960，总共 12 层的参数量 7080960 * 12 = 84971520</p>
</li>
<li>
<p>如果是预训练的话在 12 层 transformer 之后还会有分类器，这部分就忽略不计算了。</p>
</li>
<li>
<p>最后汇总 embedding 层和 12 个 transformer 层的总参数量: 16226304 + 84971520 = 101197824，也就是说大概 101M 的参数量，相比于官方所说的 110M，这其中的差距主要包括最后的分类器层，以及上述计算过程中都没有考虑偏置项（bias）。</p>
</li>
</ul>
<h2 id="2">2、根据参数量估计模型大小<a class="headerlink" href="#2" title="Permanent link">#</a></h2>
<p>比如 6B 的模型，使用 fp16 方式载入显存：
（1B = 1G）</p>
<p>6B * 2 = 12G</p>
<p>比如MOSS模型有16B参数量，使用fp16方式载入显存：
（1B = 1G）</p>
<p>16B * 2 = 32G</p>
<h2 id="3">3、根据参数量估计训练时显存大小<a class="headerlink" href="#3" title="Permanent link">#</a></h2>
<p>首先要明确显存由哪几个部分构成，这里主要参考论文 <a href="https://arxiv.org/pdf/1910.02054.pdf">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a> 中第 3 章节中的描述。</p>
<p>在训练时显卡中的第一份显存肯定要先分配给初始化环境的框架，比如 Pytorch，这部分显存的大小与显卡的型号、Pytorch 的版本号都有关系，想要确定的话直接测试一下即可。</p>
<p>除去初始化需要分配的显存以外，剩下的都是训练相关所需要的显存，将其分为两部分：</p>
<ul>
<li>
<p><code>model states</code>：包括模型的参数消耗的显存、梯度消耗的显存、优化器消耗的显存。</p>
</li>
<li>
<p><code>residual states</code>：包括前向传播时产生的中间状态消耗的显存、临时缓冲区占用的显存、以及其他零散的显存；</p>
</li>
</ul>
<p>其中 <code>model states</code> 计算起来非常明确，而 <code>residual states</code> 如何计算则不是很明确。</p>
<h3 id="31-model-states">3.1 Model States<a class="headerlink" href="#31-model-states" title="Permanent link">#</a></h3>
<p><code>model states</code> 消耗的显存包含三部分：模型的参数消耗的显存、梯度消耗的显存、优化器消耗的显存。假设模型的参数量为 <span class="arithmatex"><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span>。</p>
<p><strong>模型的参数消耗的显存</strong>：所有的模型参数需要存储到显存中，如果使用 fp16 存储，则需要的显存为 <span class="arithmatex"><span class="MathJax_Preview">2\Phi</span><script type="math/tex">2\Phi</script></span>；如果使用 fp32 存储，则需要消耗的显存为 <span class="arithmatex"><span class="MathJax_Preview">4\Phi</span><script type="math/tex">4\Phi</script></span>；</p>
<p><strong>梯度消耗的显存</strong>：梯度和模型参数的量是完全相同的，如果用 fp16 则是 <span class="arithmatex"><span class="MathJax_Preview">2\Phi</span><script type="math/tex">2\Phi</script></span>，如果用 fp32 则是 <span class="arithmatex"><span class="MathJax_Preview">4\Phi</span><script type="math/tex">4\Phi</script></span>；</p>
<p>接下来是 <strong>优化器消耗的显存</strong>，以 Adam 为例，这里还会有使用单精度（fp32）进行训练，还是使用混合精度（fp16+fp32）进行训练的区别，分别说明。</p>
<ul>
<li>
<p>如果使用单精度（fp32）进行训练，Adam 中会存储 averaged momentum 和 variance 两部分，都和模型的参数量是相同的，即 <span class="arithmatex"><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span>。</p>
<p>这样，模型的参数消耗的显存、梯度消耗的显存、优化器消耗的显存分别为：<span class="arithmatex"><span class="MathJax_Preview">4\Phi</span><script type="math/tex">4\Phi</script></span>、<span class="arithmatex"><span class="MathJax_Preview">4\Phi</span><script type="math/tex">4\Phi</script></span>、<span class="arithmatex"><span class="MathJax_Preview">4\Phi * 2</span><script type="math/tex">4\Phi * 2</script></span>。以 1.5B 的 GPT-2 为例，总大小为 1.5B * (4 + 4 + 4 * 2) = 24G</p>
</li>
<li>
<p>如果使用混合精度（fp16+fp32）进行训练，模型参数和梯度都是使用 fp16 存储，Adam 中会使用 fp32 存储一份模型参数的备份，并且使用 fp32 存储 averaged momentum 和 variance 两部分。</p>
<p>这样，模型的参数消耗的显存、梯度消耗的显存、优化器消耗的显存分别为：<span class="arithmatex"><span class="MathJax_Preview">2\Phi</span><script type="math/tex">2\Phi</script></span>、<span class="arithmatex"><span class="MathJax_Preview">2\Phi</span><script type="math/tex">2\Phi</script></span>、<span class="arithmatex"><span class="MathJax_Preview">4\Phi * 3</span><script type="math/tex">4\Phi * 3</script></span>。以 1.5B 的 GPT-2 为例，总大小为 1.5B * (2 + 2 + 4 * 3) = 24G</p>
</li>
</ul>
<p>也就是说在 <code>model states</code> 这部分，单精度（fp32）和混合精度（fp16+fp32）消耗的显存是相同的。</p>
<h3 id="32-residual-states">3.2 Residual States<a class="headerlink" href="#32-residual-states" title="Permanent link">#</a></h3>
<p><code>residual states</code> 消耗的显存包含三部分：前向传播时产生的中间状态消耗的显存、临时缓冲区占用的显存、以及其他零散的显存。</p>
<p>其中 "临时缓冲区占用的显存" 和 "其他零散的显存" 与具体的实现有很大的相关性，基本上是分析不出来的，不做分析。</p>
<p>仅看 "前向传播时产生的中间状态消耗的显存" 这部分，还不清楚具体如何计算，仅记录一个结论，该结论出自论文 <a href="https://arxiv.org/pdf/1910.02054.pdf">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a> 第 8 页的备注，如下图：</p>
<table>
<thead>
<tr>
<th>图1</th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="" src="/rJXF8VxX/01.png" /></td>
</tr>
</tbody>
</table>
<p>结论："前向传播时产生的中间状态消耗的显存" 与 "隐层维度 * batch size * seq_length * transformer 层数" 成正相关。</p>
<p>比如参数量 1.5B 的 GPT-2，其前向传播时产生的中间状态消耗的显存为 "12 * 隐层维度 * batch size * seq_length * transformer 层数"（这里的系数 12 是怎么得出来的不清楚）。将 1.5B 的 GPT-2 各参数代入到该式可以算出大概为 60G。</p>
<p>需要明确的是：上述结论是基于比较古老的计算方式得出来的，由于 "前向传播时产生的中间状态消耗的显存" 与具体的实现有着比较大的关系，所以不同的实现方式下这部分消耗的显存是不同的。比如 <a href="https://arxiv.org/pdf/1910.02054.pdf">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a> 中提到的 activation checkpointing 技术就可以显著降低这部分消耗的显存。</p>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">#</a></h2>
<ul>
<li><a href="https://arxiv.org/pdf/1910.02054.pdf">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></li>
</ul></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
