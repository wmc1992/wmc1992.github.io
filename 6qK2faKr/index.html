<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/006_LLM/999_%E5%85%B6%E4%BB%96/ColossalAI%E7%9A%84RLHF%E6%A1%86%E6%9E%B6/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Index - 算法工程师笔记</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../../css/extra.css" rel="stylesheet">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#colossalairlhf" class="nav-link">运行ColossalAI的RLHF框架</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#_1" class="nav-link">目录结构简单说明</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#rewardmodel" class="nav-link">RewardModel使用的数据集</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#rewardmodel_1" class="nav-link">RewardModel训练</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#rlprompt" class="nav-link">RL训练（随机生成prompt）</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#rlprompt_1" class="nav-link">RL训练（使用prompt数据集训练）</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#_2" class="nav-link">关于显存的消耗</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p align="right">[<a href="/6qK2faKr_no_toc/">隐藏左侧目录栏</a>][<a href="/6qK2faKr/">显示左侧目录栏</a>]</p>

<h1 id="colossalairlhf">运行ColossalAI的RLHF框架<a class="headerlink" href="#colossalairlhf" title="Permanent link">#</a></h1>
<p>ColossalAI项目链接：<a href="https://github.com/hpcaitech/ColossalAI">https://github.com/hpcaitech/ColossalAI</a></p>
<h2 id="_1">目录结构简单说明<a class="headerlink" href="#_1" title="Permanent link">#</a></h2>
<p>主要的目录结构如下图所示，<code>examples</code> 目录下是一些样例的运行脚本，<code>chatgpt</code> 目录下是RLHF的核心代码。</p>
<ul>
<li>
<p><code>train_reward_model.py</code>: 是用于训练reward model的训练脚本；</p>
</li>
<li>
<p><code>train_prompts.py</code>: 运行该脚本需要传入一个prompt数据集，该脚本并没有载入上一步中已经训练好的reward model；</p>
</li>
<li>
<p><code>train_dummy.py</code>: 运行该脚本不需要传入一个prompt数据集，因为该脚本中会从tokenizer中随机采样生成随机的prompt，这是用于测试能否跑通的一个脚本。同样的，该脚本没有载入上一步中已经训练好的reward model；</p>
</li>
</ul>
<pre><code>ColossalAI/applications/ChatGPT
├── chatgpt
│   ├── dataset
│   ├── experience_maker
│   ├── nn
│   ├── replay_buffer
│   └── trainer
├── examples
│   ├── train_dummy.py
│   ├── train_prompts.py
│   └── train_reward_model.py
├── ...
└── requirements.txt
</code></pre>
<h2 id="rewardmodel">RewardModel使用的数据集<a class="headerlink" href="#rewardmodel" title="Permanent link">#</a></h2>
<p>数据集为 <code>Dahoas/rm-static</code>，是一个英文数据集；链接为：<a href="https://huggingface.co/datasets/Dahoas/rm-static/discussions">https://huggingface.co/datasets/Dahoas/rm-static/discussions</a>；</p>
<p>使用如下代码看一下这个数据集的结构：</p>
<pre><code class="language-python">import json
from datasets import load_dataset

data = load_dataset(&quot;Dahoas/rm-static&quot;)
print(data)

for item in data[&quot;train&quot;]:
    print(json.dumps(item, indent=2))
    break
</code></pre>
<p>输出结果如下。可以看出总数据量还是非常大的，有8万多的数据量。</p>
<pre><code>DatasetDict({
    test: Dataset({
        features: ['prompt', 'response', 'chosen', 'rejected'],
        num_rows: 5103
    })
    train: Dataset({
        features: ['prompt', 'response', 'chosen', 'rejected'],
        num_rows: 76256
    })
})
{
  &quot;prompt&quot;: &quot;\n\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\n\nAssistant Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you\u2019ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\n\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\n\nAssistant:&quot;,
  &quot;response&quot;: &quot; Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it\u2019s lint-free.&quot;,
  &quot;chosen&quot;: &quot; Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it\u2019s lint-free.&quot;,
  &quot;rejected&quot;: &quot; Yes, you can spray it directly onto the cloth.&quot;
}
</code></pre>
<h2 id="rewardmodel_1">RewardModel训练<a class="headerlink" href="#rewardmodel_1" title="Permanent link">#</a></h2>
<p>训练 reward model 使用的脚本是 <code>applications/ChatGPT/examples/train_reward_model.py</code>。目前ColossalAI的最新发布版本是v0.2.5，目前在该脚本中只支持使用BLOOM作为预训练进行 reward model 的训练，做如下修改将GPT2模型和OPT模型都添加上：</p>
<p>原代码：</p>
<pre><code class="language-python"># configure model
tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)
tokenizer.pad_token = tokenizer.eos_token
model = BLOOMRM(pretrained=args.pretrain).cuda()
max_len = 1024
</code></pre>
<p>修改为：</p>
<pre><code class="language-python"># configure model
if args.model == 'gpt2':
    tokenizer = GPT2Tokenizer.from_pretrained(args.pretrain)
    tokenizer.pad_token = tokenizer.eos_token
    model = GPTRM(pretrained=args.pretrain).cuda()
elif args.model == 'bloom':
    tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)
    tokenizer.pad_token = tokenizer.eos_token
    model = BLOOMRM(pretrained=args.pretrain).cuda()
elif args.model == 'opt':
    tokenizer = AutoTokenizer.from_pretrained(args.pretrain)
    model = OPTRM(pretrained=args.pretrain).cuda()
else:
    raise ValueError(f'Unsupported model &quot;{args.model}&quot;')
max_len = 1024
</code></pre>
<blockquote>
<p>注：一些import可能也要修改，这里没有体现，缺什么补什么即可。</p>
</blockquote>
<p>训练的启动命令为：</p>
<pre><code>cd applications/ChatGPT
CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python3 examples/train_reward_model.py \
    --model gpt2 \
    --pretrain sshleifer/tiny-gpt2 \
    --dataset Dahoas/rm-static
</code></pre>
<p>这里只是简单的设置了几个参数，其他的都是使用的默认值：</p>
<ul>
<li><code>model</code>: 使用哪个模型结构，可以选值有gpt2、bloom、opt；</li>
<li><code>pretrain</code>: 具体使用哪个预训练好的模型，这个要和上面的模型结构对应起来，这里选用的一个gpt2的tiny模型，模型文件大小只有2.5M；</li>
<li><code>dataset</code>: 数据集，上一小节已经介绍过这个数据集了；</li>
</ul>
<h2 id="rlprompt">RL训练（随机生成prompt）<a class="headerlink" href="#rlprompt" title="Permanent link">#</a></h2>
<p>关于RL的训练有两个脚本：</p>
<pre><code>applications/ChatGPT/examples/train_dummy.py
applications/ChatGPT/examples/train_prompts.py
</code></pre>
<p>这两个脚本的区别在于：</p>
<ul>
<li>train_prompts.py需要传入一个prompt数据集；</li>
<li>train_dummy.py则不需要，该脚本中会从tokenizer中随机采样生成随机的prompt；</li>
</ul>
<p>这里测试使用的是脚本 <code>train_dummy.py</code>；</p>
<p>训练的启动命令如下：</p>
<pre><code>CUDA_VISIBLE_DEVICES=1 PYTHONPATH=. python3 examples/train_dummy.py \
    --model gpt2 \
    --pretrain sshleifer/tiny-gpt2
</code></pre>
<p>这里仅设置了2个参数，其他参数都是使用的默认值：</p>
<ul>
<li><code>model</code>: 使用哪个模型结构，可以选值有gpt2、bloom、opt；</li>
<li><code>pretrain</code>: 具体使用哪个预训练好的模型，这个要和上面的模型结构对应起来，这里选用的一个gpt2的tiny模型，模型文件大小只有2.5M；</li>
</ul>
<p>另外，目前的脚本 <code>applications/ChatGPT/examples/train_dummy.py</code> 没有保存模型的功能，所以要想保存模型可以自己添加上相应的代码。</p>
<h2 id="rlprompt_1">RL训练（使用prompt数据集训练）<a class="headerlink" href="#rlprompt_1" title="Permanent link">#</a></h2>
<p>训练RL时使用的prompt数据集为 <code>fka/awesome-chatgpt-prompts</code>，是一个英文数据集；链接为: <a href="https://huggingface.co/datasets/fka/awesome-chatgpt-prompts">https://huggingface.co/datasets/fka/awesome-chatgpt-prompts</a></p>
<p>这个数据集非常简单，就是一百多个prompt；</p>
<p>虽然对RL不是太熟，不过从ColossalAI给定这个图中可以知道训练RL要同时载入四个模型到显存中，这个对显存的消耗肯定是非常大的，所以我们还是使用上面已经用过的gpt2的tiny模型。</p>
<p><img alt="" src="/6qK2faKr/assets/1.jpg" /></p>
<p>训练的启动命令如下：</p>
<pre><code>CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. python3 examples/train_prompts.py ./prompts.csv \
    --model gpt2 \
    --pretrain sshleifer/tiny-gpt2
</code></pre>
<p>这里仅设置了3个参数，其他参数都是使用的默认值：</p>
<ul>
<li>训练时需要使用的prompt数据集的路径，这个把上一段中提到的数据集下载下来放到相应的路径下即可；</li>
<li><code>model</code>: 使用哪个模型结构，可以选值有gpt2、bloom、opt；</li>
<li><code>pretrain</code>: 具体使用哪个预训练好的模型，这个要和上面的模型结构对应起来，这里选用的一个gpt2的tiny模型，模型文件大小只有2.5M；</li>
</ul>
<p>另外，目前的脚本 <code>applications/ChatGPT/examples/train_prompts.py</code> 没有保存模型的功能，所以要想保存模型可以自己添加上相应的代码。</p>
<h2 id="_2">关于显存的消耗<a class="headerlink" href="#_2" title="Permanent link">#</a></h2>
<p>在上述的实验中，reward model和actor model选用的都是一个gpt2的tiny模型，存储到磁盘上只有2.5M，但是在训练RL时显存消耗达到了10G+，这个对显存的要求非常高。</p>
<p>ColossalAI项目中也有一个图片，如下所示。只看使用PyTorch框架的：</p>
<ul>
<li>小于等于10G显存的设备跑不起来；</li>
<li>40G显存的设备可以跑0.34B的模型；</li>
<li>80G显存的设备可以跑0.78B的模型；</li>
</ul>
<p>GPT-2有1.3B的参数量，按照这张图来看直接在80G的显存上是跑不起来的。</p>
<p>不知道ColossalAI的实际效果有没有他这张图里这么好，之后可以研究一下。</p>
<p><img alt="" src="/6qK2faKr/assets/3.jpg" /></p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="../../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
