<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/006_LLM/004_%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84/01_GPT2/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Index - 算法工程师笔记</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../../css/extra.css" rel="stylesheet">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#gpt2" class="nav-link">GPT2 模型结构和实现代码</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#reference" class="nav-link">Reference</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p align="right">[<a href="/wiyOMRd9_no_toc/">隐藏左侧目录栏</a>][<a href="/wiyOMRd9/">显示左侧目录栏</a>]</p>

<h1 id="gpt2">GPT2 模型结构和实现代码<a class="headerlink" href="#gpt2" title="Permanent link">#</a></h1>
<p>GPT2 的模型结构相比于 GPT 是有着少许改变的，后续的 GPT3 等模型也是在 GPT2 的基础上改出来的，所以详细了解 GPT2 的整个模型结构的细节是很有必要的。</p>
<p>在huggingface的项目 <a href="https://github.com/huggingface/transformers">transformers</a> 中对 GPT2 有着完整的实现，不过该项目毕竟是一个庞大的工程项目，里面有着各种功能的代码，不利于阅读。下面的代码是从 <a href="https://github.com/huggingface/transformers">transformers</a> 中的 GPT2 的实现中仅把模型结构部分的代码摘取出来之后的结果。需要注意的是，下述代码仅用于学习理解，其在语法上有着不少的bug，是跑不通的；同时该代码不一定是最优雅的实现，而是尽量选择容易理解的实现方式。</p>
<p>模型中涉及的几个公式如下所示：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{Attention}(x) = \text{softmax}(\frac{xW_q \cdot xW_k}{\sqrt{d_k}}) \cdot xW_v\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{Attention}(x) = \text{softmax}(\frac{xW_q \cdot xW_k}{\sqrt{d_k}}) \cdot xW_v\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{FFN}(x) = \text{act}(xW_1)W_2\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{FFN}(x) = \text{act}(xW_1)W_2\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\text{transformer}(x) = \text{residual}(\text{FFN}(\text{LN}(\text{residual}(\text{Attention}(\text{LN}(x))))))\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\text{transformer}(x) = \text{residual}(\text{FFN}(\text{LN}(\text{residual}(\text{Attention}(\text{LN}(x))))))\end{equation}</script>
</div>
<p>上述公式中 <span class="arithmatex"><span class="MathJax_Preview">\text{act}(\cdot)</span><script type="math/tex">\text{act}(\cdot)</script></span> 表示激活函数。</p>
<p>下述代码中有四个类：</p>
<ul>
<li>
<p><code>GTP2Attention</code>: 该类实现的功能是 multi-head attention 部分的功能；</p>
</li>
<li>
<p><code>GPT2MLP</code>: 该类实现的功能是 FFN 部分的功能；</p>
</li>
<li>
<p><code>GPT2Block</code>: 该类堆叠了 multi-head attention 和 FFN，也就是实现了 transformer 层；</p>
</li>
<li>
<p><code>GPT2Model</code>: 该类是堆叠了 embedding 层和多个 tranformer 层，也就是一个完整的 decoder-only 的模型结构；</p>
</li>
</ul>
<p>主要部分的代码都已经有了注释，比较容易理解：</p>
<pre><code class="language-python">import torch
from torch import nn


class GTP2Attention(nn.Module):
    &quot;&quot;&quot; 该类为一个完整的 multi-head attention 结构 &quot;&quot;&quot;

    def __init__(self, config):
        super().__init__()

        self.embed_dim = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.embed_dim // self.num_heads
        self.split_size = self.embed_dim

        self.c_attn = nn.Linear(self.embed_dim, 3 * self.embed_dim)
        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)

        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        self.resid_dropout = nn.Dropout(config.resid_pdrop)

    def forward(self, hidden_states, attention_mask=None):
        # hidden_states: [bs, seq_len, embed_dim]

        # 将 W_q, W_k, W_v 三个矩阵合成一次矩阵乘法运算，乘完之后再分割开来
        # query: [bs, seq_len, embed_dim]
        # key:   [bs, seq_len, embed_dim]
        # value: [bs, seq_len, embed_dim]
        query, key, value = self.c_attn(hidden_states).split(self.embed_dim, dim=2)

        # [bs, seq_len, embed_dim] -&gt; [bs, seq_len, num_heads, head_dim] -&gt; [bs, num_heads, seq_len, head_dim]
        new_size = query.size()[:-1] + [self.num_heads, self.head_dim]
        query, key, value = query.view(new_size), key.view(new_size), value.view(new_size)
        query, key, value = query.permute(0, 2, 1, 3), key.permute(0, 2, 1, 3), value.permute(0, 2, 1, 3)

        # 注意力权重矩阵att_weights: [bs, num_heads, seq_len, seq_len]
        att_weights = torch.matmul(query, key.transpose(-1, -2))

        # 对注意力矩阵除上一个根号d_k，然后再做softmax
        att_weights = att_weights / torch.full([], value.size()[-1] ** 0.5)
        if attention_mask is not None:
            att_weights = att_weights + attention_mask
        att_weights = nn.functional.softmax(att_weights, dim=-1)
        att_weights = self.attn_dropout(att_weights)

        # att_output: [bs, num_heads, seq_len, head_dim] = [bs, num_heads, seq_len, seq_len] * [bs, num_heads, seq_len, head_dim]
        att_output = torch.matmul(att_weights, value)

        # 注意：这里不能够直接将 att_output 由 [bs, num_heads, seq_len, head_dim] 变为 [bs, seq_len, embed_dim]
        # [bs, num_heads, seq_len, head_dim] -&gt; [bs, seq_len, num_heads, head_dim] -&gt; [bs, seq_len, embed_dim]
        att_output = att_output.permute(0, 2, 1, 3)
        att_output = att_output.view(att_output.size()[:-2] + [self.num_heads * self.head_dim, ])

        # 
        att_output = self.c_proj(att_output)
        att_output = self.resid_dropout(att_output)

        return att_output


class GPT2MLP(nn.Module):
    &quot;&quot;&quot; 该类为一个完整的 FFN 结构 &quot;&quot;&quot;

    def __init__(self, intermediate_size, config):
        super().__init__()

        self.embed_dim = config.hidden_size

        # 一般情况下 intermediate_size = 4 * self.embed_dim，所以这里是一个先升维，后降维的过程
        self.c_fc = nn.Linear(self.embed_dim, intermediate_size)
        self.c_proj = nn.Linear(intermediate_size, self.embed_dim)
        self.act = ... # TODO 这里初始化一个激活函数，比如 ReLU、GELU 等
        self.dropout = nn.Dropout(config.resid_pdrop)

    def forward(self, hidden_states, ):
        hidden_states = self.c_fc(hidden_states)
        hidden_states = self.act(hidden_states)
        hidden_states = self.c_proj(hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states


class GPT2Block(nn.Module):
    &quot;&quot;&quot; 该类为一个完整的 transformer 结构 &quot;&quot;&quot;

    def __init__(self, config):
        super().__init__()

        hidden_size = config.hidden_size
        inner_dim = 4 * hidden_size

        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        self.att = GTP2Attention(config)
        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        self.mlp = GPT2MLP(inner_dim, config)

    def forward(self, hidden_states, attention_mask=None):
        # ------------------------------------------------------
        # multi-head attention 部分
        # ------------------------------------------------------
        # 保存一下 multi-head attention 层的输入值，等会用于残差连接
        residual = hidden_states
        # 前置（pred）layer norm
        hidden_states = self.ln_1(hidden_states)
        attn_outputs = self.att(hidden_states, attention_mask)
        # 残差连接
        hidden_states = attn_outputs + residual

        # ------------------------------------------------------
        # FFN 部分
        # ------------------------------------------------------
        # 保存一下 FFN 层的输入值，等会用于残差连接
        residual = hidden_states
        # 前置（pred）layer norm
        hidden_states = self.ln_2(hidden_states)
        feed_forward_hidden_states = self.mlp(hidden_states)
        # 残差连接
        hidden_states = residual + feed_forward_hidden_states

        return hidden_states


class GPT2Model(nn.Module):
    &quot;&quot;&quot; 该类是堆叠 embedding 层以及多个 transformer 层 &quot;&quot;&quot;

    def __init__(self, config):
        super().__init__()

        self.embed_dim = config.hidden_size

        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)
        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)
        self.drop = nn.Dropout(config.embd_pdrop)

        self.h = nn.ModuleList([GPT2Block(config) for _ in range(config.num_hidden_layers)])
        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)

    def forward(self, input_ids, attention_mask, ):
        input_shape = input_ids.size()
        position_ids = torch.arange(input_shape[-1]).unsqueeze(0)

        # Embedding层，这里只相加了token embedding和position embeddding，忽略了token type embedding
        inputs_embed = self.wte(input_ids)
        position_embed = self.wpe(position_ids)
        # inputs_embed是[bs, seq_len, embed_dim]，position_embed是[1, seq_len, embed_dim]，这里相加时会做广播
        hidden_states = inputs_embed + position_embed
        hidden_states = self.drop(hidden_states)

        # 经过多个transformer层
        for block in self.h:
            hidden_states = block(hidden_states, attention_mask)

        # 由于tranformer中使用的是pre layer norm，所以最后还需要过一层layer norm
        hidden_states = self.ln_f(hidden_states)
        return hidden_states
</code></pre>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">#</a></h2>
<ul>
<li><a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></li>
</ul></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="../../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
