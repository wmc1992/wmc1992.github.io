<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/006_LLM/002_%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/007_LLaMA2%E6%A8%A1%E5%9E%8B/LLaMA2ChatModel%E6%8E%A8%E7%90%86%E4%BB%A3%E7%A0%81%E8%AF%B4%E6%98%8E/">
        <link rel="shortcut icon" href="../../../../img/favicon.ico">
        <title>Index - 算法工程师笔记</title>
        <link href="../../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../../../css/extra.css" rel="stylesheet">

        <script src="../../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../../../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#llama2-chat-model" class="nav-link">LLaMA2-Chat-Model 推理代码</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#1" class="nav-link">1、对话数据构造规则说明</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#2" class="nav-link">2、推理代码</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#3" class="nav-link">3、完整推理代码</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#reference" class="nav-link">Reference</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p align="right">[<a href="/zwYQyMpu_no_toc/">隐藏左侧目录栏</a>][<a href="/zwYQyMpu/">显示左侧目录栏</a>]</p>

<h1 id="llama2-chat-model">LLaMA2-Chat-Model 推理代码<a class="headerlink" href="#llama2-chat-model" title="Permanent link">#</a></h1>
<h2 id="1">1、对话数据构造规则说明<a class="headerlink" href="#1" title="Permanent link">#</a></h2>
<p>官方给出的推理代码的脚本为 <a href="https://github.com/facebookresearch/llama/blob/main/llama/generation.py">generation.py</a>，该脚本中对于对话数据的构造过程说明如下。</p>
<p>LLaMA2 会将对话数据构造为如下形式：</p>
<pre><code>&lt;bos&gt; B_INST B_SYS SytemPrompt E_SYS UserPrompt E_INST Answer &lt;eos&gt;
</code></pre>
<p>当对话有多轮时继续在后面添加，也就是如下形式：</p>
<pre><code>&lt;bos&gt; B_INST B_SYS SytemPrompt E_SYS UserPrompt1 E_INST Answer1 &lt;eos&gt; &lt;bos&gt; B_INST UserPrompt2 E_INST Answer2 &lt;eos&gt; &lt;bos&gt; B_INST UserPrompt3 E_INST Answer3 &lt;eos&gt;
</code></pre>
<p>上面的 B_INST、E_INST、B_SYS、E_SYS 都是一些特殊的字符，如下：</p>
<pre><code class="language-python">B_INST = &quot;[INST]&quot;
E_INST = &quot;[/INST]&quot;
B_SYS = &quot;&lt;&lt;SYS&gt;&gt;\n&quot;
E_SYS = &quot;\n&lt;&lt;/SYS&gt;&gt;\n\n&quot;
</code></pre>
<p>如果用户没有指定 system prompt，会给自动加上模型的 system prompt；如果用户自己指定了，则使用用户指定的 prompt。默认的 system prompt 为：</p>
<pre><code class="language-python">DEFAULT_SYSTEM_PROMPT = &quot;&quot;&quot;You are a helpful, respectful and honest assistant. Always answer as helpfully as \
possible, while being safe. Your answers should not include any harmful, unethical, \
racist, sexist, toxic, dangerous, or illegal content. Please ensure that your \
responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why \
instead of answering something not correct. If you don't know the answer to a question, \
please don't share false information.&quot;&quot;&quot;
</code></pre>
<p>上面是一些规则，下面举两个🌰说明一下。第一组是一个单轮对话的🌰，并且用户自己指定了 system prompt，如下：</p>
<pre><code class="language-json">[
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Always answer with emojis&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How to go from Beijing to NY?&quot;},
]
</code></pre>
<p>其构造出来的模型输入为如下形式：</p>
<pre><code>&lt;s&gt; [INST] &lt;&lt;SYS&gt;&gt;
Always answer with emojis
&lt;&lt;/SYS&gt;&gt;

How to go from Beijing to NY? [/INST]
</code></pre>
<p>第二组是一个多轮对话的🌰，并且用户没有自己指定 system prompt，如下：</p>
<pre><code class="language-json">[
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Hello! I'm just an AI, my purpose is to assist and ... and I will do my best to assist you.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;那你会说中文吗？&quot;},
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Yes, I can speak Chinese. I'm just an AI, I don't have a physical voice, but I can communicate in Chinese text. Would you like to chat in Chinese?&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;是的，接下来请用中文聊天。&quot;},
]
</code></pre>
<p>其构造出来的模型输入为如下形式：</p>
<pre><code>&lt;s&gt; [INST] &lt;&lt;SYS&gt;&gt;
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
&lt;&lt;/SYS&gt;&gt;

Who are you? [/INST] Hello! I'm just an AI, my purpose is to assist and ... and I will do my best to assist you. &lt;/s&gt;&lt;s&gt; [INST] 那你会说中文吗？ [/INST] Yes, I can speak Chinese. I'm just an AI, I don't have a physical voice, but I can communicate in Chinese text. Would you like to chat in Chinese? &lt;/s&gt;&lt;s&gt; [INST] 是的，接下来请用中文聊天。 [/INST]
</code></pre>
<blockquote>
<p>PS: 对于这个例子，虽然模型说它可以使用中文，但是实际上还是只输出英文。整理测试的效果是：可以理解中文，但是不会输出中文。</p>
</blockquote>
<h2 id="2">2、推理代码<a class="headerlink" href="#2" title="Permanent link">#</a></h2>
<p>下面基于 huggingface 的各种框架说明一下 LLaMA2-Chat-Model 的推理代码。</p>
<p>在类 <code>LlamaTokenizer</code> 中新增了一个函数 <a href="https://github.com/huggingface/transformers/blob/v4.31.0/src/transformers/models/llama/tokenization_llama.py#L334">_build_conversation_input_ids()</a>，该函数就是用于实现上述对话数据构造规则的。该函数的输入是一个 <a href="https://github.com/huggingface/transformers/blob/v4.31.0/src/transformers/pipelines/conversational.py#L18">Conversation</a> 类型的对象，这个类非常简单，先说明一下这个类。</p>
<pre><code>Conversation(text, past_user_inputs=[], generated_responses=[])
</code></pre>
<p>如上述代码所示，创建该类时传入三个参数：</p>
<ul>
<li><code>past_user_inputs</code>: 是一个list类型，是历史聊天过程中，user 的所有提问；</li>
<li><code>generated_responses</code>: 是一个list类型，是历史聊天过程中，assistant 的所有回答；</li>
<li><code>text</code>: str类型，是最新一次的 user 的提问；</li>
</ul>
<p>要注意，字段 <code>past_user_inputs</code> 与字段 <code>generated_responses</code> 必须是一一对应的，构造时是 <code>zip(past_user_inputs, generated_responses)</code> 这么对应的。</p>
<p>看完 Conversation 这个类的三个字段就会发现这个类里面没有给 system prompt 预留一个字段，所以在函数 <a href="https://github.com/huggingface/transformers/blob/v4.31.0/src/transformers/models/llama/tokenization_llama.py#L334">_build_conversation_input_ids()</a> 用了一个折衷的方法，它是检测列表 <code>past_user_inputs</code> 中的第一个元素是否包含 <code>B_SYS</code> 或者 <code>E_SYS</code> 来判断用户是否自己指定了 system prompt。所以如果想要自己指定 system prompt，在创建对象 Conversation 之前需要自己处理一下，代码如下：</p>
<pre><code class="language-python">B_SYS, E_SYS = &quot;&lt;&lt;SYS&gt;&gt;\n&quot;, &quot;\n&lt;&lt;/SYS&gt;&gt;\n\n&quot;

# dialog 举例: [
#     {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Always answer with emojis&quot;},
#     {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How to go from Beijing to NY?&quot;},
# ]
if dialog[0][&quot;role&quot;] == &quot;system&quot;:  # 把自己指定的 system prompt 合并到 user 的第一次的问题中
    dialog = [
        {
            &quot;role&quot;: dialog[1][&quot;role&quot;],
            &quot;content&quot;: B_SYS + dialog[0][&quot;content&quot;] + E_SYS + dialog[1][&quot;content&quot;],
        }
    ] + dialog[2:]

conversation = Conversation(
    text=dialog[-1][&quot;content&quot;],
    past_user_inputs=[item[&quot;content&quot;] for item in dialog[:-1] if item[&quot;role&quot;] == &quot;user&quot;],
    generated_responses=[item[&quot;content&quot;] for item in dialog[:-1] if item[&quot;role&quot;] == &quot;assistant&quot;]
)
</code></pre>
<p>相对完整的推理代码片段如下：</p>
<pre><code class="language-python">import torch
from transformers.pipelines.conversational import Conversation

model = ...
tokenizer = ...

dialog = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Always answer with Haiku&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I am going to Paris, what should I see?&quot;},
]

# 构造成conversation格式数据

B_SYS, E_SYS = &quot;&lt;&lt;SYS&gt;&gt;\n&quot;, &quot;\n&lt;&lt;/SYS&gt;&gt;\n\n&quot;
if dialog[0][&quot;role&quot;] == &quot;system&quot;:  # 把自己指定的 system prompt 合并到 user 的第一次的问题中
    dialog = [
        {
            &quot;role&quot;: dialog[1][&quot;role&quot;],
            &quot;content&quot;: B_SYS + dialog[0][&quot;content&quot;] + E_SYS + dialog[1][&quot;content&quot;],
        }
    ] + dialog[2:]

# 检查 dialog 是否是 (u/a/u/a/u...) 的格式
assert all([msg[&quot;role&quot;] == &quot;user&quot; for msg in dialog[::2]]) and all(
    [msg[&quot;role&quot;] == &quot;assistant&quot; for msg in dialog[1::2]]
), (
    &quot;model only supports 'system', 'user' and 'assistant' roles, &quot;
    &quot;starting with 'system', then 'user' and alternating (u/a/u/a/u...)&quot;
)
assert (dialog[-1][&quot;role&quot;] == &quot;user&quot;), f&quot;Last message must be from user, got {dialog[-1]['role']}&quot;

conversation = Conversation(
    text=dialog[-1][&quot;content&quot;],
    past_user_inputs=[item[&quot;content&quot;] for item in dialog[:-1] if item[&quot;role&quot;] == &quot;user&quot;],
    generated_responses=[item[&quot;content&quot;] for item in dialog[:-1] if item[&quot;role&quot;] == &quot;assistant&quot;]
)

# 推理
with torch.no_grad():
    input_ids = tokenizer._build_conversation_input_ids(conversation)
    input_ids = torch.tensor([input_ids])

    out = model.generate(
        input_ids=input_ids.cuda(),
        max_new_tokens=512,
        top_p=0.9,
        temperature=0.6,
    )

    out_text = tokenizer.decode(out[0])
    print(out_text)
    print(&quot;=&quot; * 100)
</code></pre>
<h2 id="3">3、完整推理代码<a class="headerlink" href="#3" title="Permanent link">#</a></h2>
<p>详见: <a href="https://github.com/wmc1992/transformers_add_lora/blob/master/core/llama2/inference_with_qlora.py">https://github.com/wmc1992/transformers_add_lora/blob/master/core/llama2/inference_with_qlora.py</a></p>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">#</a></h2>
<ul>
<li>
<p><a href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a></p>
</li>
<li>
<p><a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p>
</li>
<li>
<p><a href="https://github.com/artidoro/qlora">https://github.com/artidoro/qlora</a></p>
</li>
</ul></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../../js/base.js" defer></script>
        <script src="../../../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
