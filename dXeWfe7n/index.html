<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="mingchao.wang">
        <link rel="canonical" href="https://mingchao.wang/006_LLM/007_RL/02_%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Index - 算法工程师笔记</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/obsidian.min.css">
        <link href="../../../css/extra.css" rel="stylesheet">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-274394082');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../../..">算法工程师笔记</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/wmc1992/" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#_1" class="nav-link">马尔可夫随机过程、奖励过程、决策过程</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#1" class="nav-link">1、随机过程</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#2mp" class="nav-link">2、马尔可夫随机过程(MP)</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#21" class="nav-link">2.1 马尔可夫性质</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#22" class="nav-link">2.2 马尔可夫随机过程</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#3mrp" class="nav-link">3、马尔可夫奖励过程(MRP)</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#31" class="nav-link">3.1 奖励过程</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#32" class="nav-link">3.2 回报</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#33" class="nav-link">3.3 价值函数</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#34" class="nav-link">3.4 贝尔曼方程</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#4mdp" class="nav-link">4、马尔可夫决策过程(MDP)</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="3"><a href="#41" class="nav-link">4.1 决策过程</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#42" class="nav-link">4.2 策略</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#43" class="nav-link">4.3 状态价值函数</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#44" class="nav-link">4.4 动作价值函数</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="3"><a href="#45" class="nav-link">4.5 贝尔曼期望函数</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#reference" class="nav-link">Reference</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p align="right">[<a href="/dXeWfe7n_no_toc/">隐藏左侧目录栏</a>][<a href="/dXeWfe7n/">显示左侧目录栏</a>]</p>

<h1 id="_1">马尔可夫随机过程、奖励过程、决策过程<a class="headerlink" href="#_1" title="Permanent link">#</a></h1>
<blockquote>
<p>重要！！！</p>
<p>涉及到马尔可夫过程，就必须要强调的两个概念：一个概念是所有状态的集合，一个概念是时间序列。</p>
<ul>
<li>
<p>所有状态的集合使用数学符号 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{S} = \{s_1, s_2, ..., s_n\}</span><script type="math/tex">\mathcal{S} = \{s_1, s_2, ..., s_n\}</script></span>，其表示可以取到的状态值。在本文中其下标一般使用 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>，表示总共有 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 个状态值。</p>
</li>
<li>
<p>时间序列使用数学符号 <span class="arithmatex"><span class="MathJax_Preview">(S_1, S_2, ..., S_t, ..., S_T)</span><script type="math/tex">(S_1, S_2, ..., S_t, ..., S_T)</script></span>，其表示从初始时刻开始到 <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> 时刻做了一次采样之后得到的结果。在本文中其下标一般使用 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>，表示在第 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 时刻所取到的状态。</p>
</li>
</ul>
<p>在本文中，表示某一个状态值时使用小写的 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 来表示，表示某个时刻的状态时使用大写的 <span class="arithmatex"><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> 来表示。</p>
<p>刚开始看马尔可夫过程时，对这两个概念弄混了很久。</p>
</blockquote>
<h2 id="1">1、随机过程<a class="headerlink" href="#1" title="Permanent link">#</a></h2>
<p>概率论的研究对象是静态的随机对象。随机过程（stochastic process）的研究对象是随时间演变的随机对象。</p>
<p>在随机过程中，某一时刻 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 的状态取值使用符号 <span class="arithmatex"><span class="MathJax_Preview">S_t</span><script type="math/tex">S_t</script></span> 表示（这里的 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 是单词 state 的首字母），所有可能的状态的集合使用符号 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{S} = \{s_1, s_2, ..., s_n\}</span><script type="math/tex">\mathcal{S} = \{s_1, s_2, ..., s_n\}</script></span> 表示。</p>
<p>某个时刻 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 的状态取决于之前各个时刻的状态，也就是说，已知 <span class="arithmatex"><span class="MathJax_Preview">(S_1, S_2, ..., S_t)</span><script type="math/tex">(S_1, S_2, ..., S_t)</script></span>，那么下一时刻 <span class="arithmatex"><span class="MathJax_Preview">t+1</span><script type="math/tex">t+1</script></span> 的状态 <span class="arithmatex"><span class="MathJax_Preview">S_{t+1}</span><script type="math/tex">S_{t+1}</script></span> 的概率表示为 <span class="arithmatex"><span class="MathJax_Preview">P(S_{t+1}|S_1, S_2, ..., S_t)</span><script type="math/tex">P(S_{t+1}|S_1, S_2, ..., S_t)</script></span>。</p>
<h2 id="2mp">2、马尔可夫随机过程(MP)<a class="headerlink" href="#2mp" title="Permanent link">#</a></h2>
<h3 id="21">2.1 马尔可夫性质<a class="headerlink" href="#21" title="Permanent link">#</a></h3>
<p>马尔可夫性质（Markov property）：某一时刻的状态仅取决于上一时刻的状态，用公式表示为 <span class="arithmatex"><span class="MathJax_Preview">P(S_{t+1}|S_t) = P(S_{t+1}|S_1, S_2, ..., S_t)</span><script type="math/tex">P(S_{t+1}|S_t) = P(S_{t+1}|S_1, S_2, ..., S_t)</script></span>。</p>
<h3 id="22">2.2 马尔可夫随机过程<a class="headerlink" href="#22" title="Permanent link">#</a></h3>
<p>马尔可夫过程（Markov process）也称为马尔可夫链（Markov chain），指的是具有马尔可夫性质的随机过程。一般使用符号 <span class="arithmatex"><span class="MathJax_Preview">&lt;\mathcal{S}, \mathcal{P}&gt;</span><script type="math/tex"><\mathcal{S}, \mathcal{P}></script></span> 表示。其中 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span> 表示 <strong>有限个</strong> 状态的集合，<span class="arithmatex"><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span> 表示状态转移矩阵（state transition matrix）。</p>
<p>假设总共有 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 个状态，此时 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span> 的公式分别如下所示：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\mathcal{S}=\{s_1, s_2, ..., s_n\}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\mathcal{S}=\{s_1, s_2, ..., s_n\}\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\mathcal{P}=\begin{bmatrix}
P(s_1|s_1) &amp; P(s_2|s_1) &amp; \cdots &amp; P(s_n|s_1) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
P(s_1|s_n) &amp; P(s_2|s_n) &amp; \cdots &amp; P(s_n|s_n)
\end{bmatrix}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\mathcal{P}=\begin{bmatrix}
P(s_1|s_1) & P(s_2|s_1) & \cdots & P(s_n|s_1) \\
\vdots & \vdots & \ddots & \vdots \\
P(s_1|s_n) & P(s_2|s_n) & \cdots & P(s_n|s_n)
\end{bmatrix}\end{equation}</script>
</div>
<p>上述状态转移矩阵 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span> 中第 <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 行第 <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> 列的元素 <span class="arithmatex"><span class="MathJax_Preview">P(s_j|s_i)</span><script type="math/tex">P(s_j|s_i)</script></span> 表示从状态 <span class="arithmatex"><span class="MathJax_Preview">s_i</span><script type="math/tex">s_i</script></span> 转移到状态 <span class="arithmatex"><span class="MathJax_Preview">s_j</span><script type="math/tex">s_j</script></span> 的概率。</p>
<p>从任一状态出发，转移到所有状态的概率之和必须为1，所以在状态转移矩阵 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span> 中每一行元素之和为1。</p>
<p>对于那些只进不出的状态（在状态转移矩阵 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span> 中表现为以1的概率转移到自身），一般称其为终止状态（terminal state）。</p>
<p>给定了 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span> 之后就意味着给定了马尔可夫过程。给定了一个马尔可夫过程之后，就可以从某一个状态出发，根据其状态转移矩阵生成一个状态序列（episode）。这种操作一般称为采样（sampling）。</p>
<p>下图就是一个马尔可夫过程的例子，下图中的每个点就是一个状态，每个边对应着状态转移矩阵中的一个元素（转移概率为0的都省略了没有画出来）。可以看出下图中的 s6 节点只进不出，所以其就是一个终止节点。</p>
<div align=center><img src="/dXeWfe7n/assets/01.png" width=40% /></div>

<h2 id="3mrp">3、马尔可夫奖励过程(MRP)<a class="headerlink" href="#3mrp" title="Permanent link">#</a></h2>
<h3 id="31">3.1 奖励过程<a class="headerlink" href="#31" title="Permanent link">#</a></h3>
<p>马尔可夫奖励过程（Markov reward process）就是在马尔可夫过程的基础上增加上奖励函数 <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> 和折扣因子 <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span>。</p>
<p>某个状态的奖励函数 <span class="arithmatex"><span class="MathJax_Preview">r(s)</span><script type="math/tex">r(s)</script></span> 指的是转移到该状态可以获得的奖励的期望。</p>
<p>折扣因子（discount factor） <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 的取值范围是 <span class="arithmatex"><span class="MathJax_Preview">[0, 1)</span><script type="math/tex">[0, 1)</script></span>。引入折扣因子的作用是长期的奖励具有一定的不确定性，折扣因子的作用就是对长期的奖励打一个折扣、做一个衰减，更关注近期的奖励。<span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 越接近0说明越忽略长期奖励，<span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 越接近1说明越关注长期奖励。折扣因子是在计算下一小节所讲的 "回报" 时起的作用。</p>
<p>所以，一个马尔可夫奖励过程可以使用符号 <span class="arithmatex"><span class="MathJax_Preview">&lt;\mathcal{S}, \mathcal{P}, r, \gamma&gt;</span><script type="math/tex"><\mathcal{S}, \mathcal{P}, r, \gamma></script></span> 来表示。</p>
<p>另外，使用符号 <span class="arithmatex"><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> 表示在 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 时刻获得的奖励。举例说明一下奖励函数 <span class="arithmatex"><span class="MathJax_Preview">r(s)</span><script type="math/tex">r(s)</script></span> 与奖励 <span class="arithmatex"><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> 的区别。假设对于某个状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 其有0.8的概率获取1点奖励，有0.2的概率获得5点奖励。那么某次采样可能得到1点奖励，此时 <span class="arithmatex"><span class="MathJax_Preview">R_t=1</span><script type="math/tex">R_t=1</script></span>，另一次采样可能得到5点奖励，此时 <span class="arithmatex"><span class="MathJax_Preview">R_t=5</span><script type="math/tex">R_t=5</script></span>。而其奖励函数表示的是该状态可以获得的奖励的期望，永远为 <span class="arithmatex"><span class="MathJax_Preview">r(s)=0.8*1+0.2*5=1.8</span><script type="math/tex">r(s)=0.8*1+0.2*5=1.8</script></span>。也就是说，奖励函数 <span class="arithmatex"><span class="MathJax_Preview">r(s)</span><script type="math/tex">r(s)</script></span> 是不随时刻 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 变化、不随采样次数变化的；而奖励 <span class="arithmatex"><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> 则是某次采样的具体结果。</p>
<h3 id="32">3.2 回报<a class="headerlink" href="#32" title="Permanent link">#</a></h3>
<p>在马尔可夫奖励过程中，从 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 时刻的状态 <span class="arithmatex"><span class="MathJax_Preview">S_t</span><script type="math/tex">S_t</script></span> 开始，直到终止状态，所有奖励的衰减之和称为回报，使用符号 <span class="arithmatex"><span class="MathJax_Preview">G_t</span><script type="math/tex">G_t</script></span> 表示。其公式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}G_t = R_t + \gamma R_{t+1} + \gamma ^ 2 R_{t+2} + \cdots = \sum_{k=0}^\infty \gamma^k \cdot R_{t+k} \end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}G_t = R_t + \gamma R_{t+1} + \gamma ^ 2 R_{t+2} + \cdots = \sum_{k=0}^\infty \gamma^k \cdot R_{t+k} \end{equation}</script>
</div>
<p>其中 <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 是折扣因子，<span class="arithmatex"><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> 是在 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 时刻获得的奖励，都在上一小节介绍过了。</p>
<h3 id="33">3.3 价值函数<a class="headerlink" href="#33" title="Permanent link">#</a></h3>
<p>在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态开始未来累积奖励的期望）称为这个状态的价值（value），所有状态的价值就构成了期望函数（value function），其公式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}V(s) = E[G_t|S_t=s]\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}V(s) = E[G_t|S_t=s]\end{equation}</script>
</div>
<h3 id="34">3.4 贝尔曼方程<a class="headerlink" href="#34" title="Permanent link">#</a></h3>
<h4 id="341">3.4.1 贝尔曼方程推导<a class="headerlink" href="#341" title="Permanent link">#</a></h4>
<p>从上述价值函数出发，可以推导出贝尔曼方程（Bellman equation），推导过程如下所示：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
V(s) &amp;= E[G_t|S_t=s] \\
&amp;= E[R_t + \gamma R_{t+1} + \gamma^2 R_{t+1} + \cdots | S_t = s] \\
&amp;= E[R_t + \gamma (R_{t+1} + \gamma R_{t+1} + \cdots) | S_t = s] \\
&amp;= E[R_t + \gamma G_{t+1} | S_t = s] \\
&amp;= E[R_t + \gamma V(R_{t+1}) | S_t = s]
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
V(s) &= E[G_t|S_t=s] \\
&= E[R_t + \gamma R_{t+1} + \gamma^2 R_{t+1} + \cdots | S_t = s] \\
&= E[R_t + \gamma (R_{t+1} + \gamma R_{t+1} + \cdots) | S_t = s] \\
&= E[R_t + \gamma G_{t+1} | S_t = s] \\
&= E[R_t + \gamma V(R_{t+1}) | S_t = s]
\end{split}\end{equation}</script>
</div>
<p>上述推导过程的最后一行在刚开始看的时候完全无法理解，主要不理解的是为什么能够对价值函数 <span class="arithmatex"><span class="MathJax_Preview">V(R_{t+1})</span><script type="math/tex">V(R_{t+1})</script></span> 求期望，也就是公式 <span class="arithmatex"><span class="MathJax_Preview">E[V(R_{t+1})]</span><script type="math/tex">E[V(R_{t+1})]</script></span> 是什么意思？这里专门对最后一行的含义做一下解释。</p>
<p>公式中的 <span class="arithmatex"><span class="MathJax_Preview">[|S_t=s]</span><script type="math/tex">[|S_t=s]</script></span> 意思是在时刻 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 状态为 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>。公式中的 <span class="arithmatex"><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> 表示在时刻 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 获得的奖励。最主要是公式中的 <span class="arithmatex"><span class="MathJax_Preview">E[V(R_{t+1})]</span><script type="math/tex">E[V(R_{t+1})]</script></span> 的含义是什么。</p>
<p>从之前的定义已经知道 <span class="arithmatex"><span class="MathJax_Preview">V(s)</span><script type="math/tex">V(s)</script></span> 表示的是当前时刻 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 的状态为 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>，从当前时刻到序列结束所获得的所有回报 <span class="arithmatex"><span class="MathJax_Preview">G</span><script type="math/tex">G</script></span> 的期望。在该定义中非常重要的一点是：当前时刻 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 的状态是固定的，就是状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>，而不是状态集合 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span> 中任何其他的状态。假设现在只知道时刻 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 是状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>，对于时刻 <span class="arithmatex"><span class="MathJax_Preview">t+1</span><script type="math/tex">t+1</script></span> 以及之后的时刻是什么状态都是未知的，那么如何表示从时刻 <span class="arithmatex"><span class="MathJax_Preview">t+1</span><script type="math/tex">t+1</script></span> 直到序列结束所获得的所有回报的期望呢？可以遍历 <span class="arithmatex"><span class="MathJax_Preview">t+1</span><script type="math/tex">t+1</script></span> 时刻的所有可能的状态，求出每个状态的价值，然后就可以求出期望了，这个过程使用公式表示就为：<span class="arithmatex"><span class="MathJax_Preview">E[V(R_{t+1})]=\sum_{s^\prime \in \mathcal{S}} p(s^\prime) V(s^\prime)</span><script type="math/tex">E[V(R_{t+1})]=\sum_{s^\prime \in \mathcal{S}} p(s^\prime) V(s^\prime)</script></span>。所以符号 <span class="arithmatex"><span class="MathJax_Preview">E[V(R_{t+1})]</span><script type="math/tex">E[V(R_{t+1})]</script></span> 的含义是：在不知道 <span class="arithmatex"><span class="MathJax_Preview">t+1</span><script type="math/tex">t+1</script></span> 时刻是什么状态的情况下，从 <span class="arithmatex"><span class="MathJax_Preview">t+1</span><script type="math/tex">t+1</script></span> 时刻到序列结束所获得的所有回报 <span class="arithmatex"><span class="MathJax_Preview">G</span><script type="math/tex">G</script></span> 的期望。</p>
<p>然后根据上述推导过程的最后一行就有了贝尔曼方程，公式如下：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}V(s) = r(s) + \gamma \sum_{s^\prime \in S} p(s^\prime | s) V(s^\prime)\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}V(s) = r(s) + \gamma \sum_{s^\prime \in S} p(s^\prime | s) V(s^\prime)\end{equation}</script>
</div>
<h4 id="342">3.4.2 贝尔曼方程直观理解<a class="headerlink" href="#342" title="Permanent link">#</a></h4>
<p>从形式上来看，贝尔曼方程其实是状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 的价值和其下一个状态 <span class="arithmatex"><span class="MathJax_Preview">s^\prime</span><script type="math/tex">s^\prime</script></span> 的价值的递归式。下面从直观上说明一下贝尔曼方程表示的意思。等号左侧是状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 的价值，它等于两部分求和。一部分是当前状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 的奖励期望，另一部分是衰减后的下一状态 <span class="arithmatex"><span class="MathJax_Preview">s^\prime</span><script type="math/tex">s^\prime</script></span> 的价值的加权和。加权和中的权重就是状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 转移到状态 <span class="arithmatex"><span class="MathJax_Preview">s^\prime</span><script type="math/tex">s^\prime</script></span> 的概率。</p>
<p>对于状态集合 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span> 中的任何一个状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>，贝尔曼方程都是成立的。</p>
<h4 id="343">3.4.3 贝尔曼方程矩阵形式<a class="headerlink" href="#343" title="Permanent link">#</a></h4>
<p>下面把贝尔曼方程写成矩阵形式。假设马尔可夫奖励过程有 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 个状态，即 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{S} = \{s_1, s_2, ..., s_n\}</span><script type="math/tex">\mathcal{S} = \{s_1, s_2, ..., s_n\}</script></span>，将所有状态的价值表示成一个列向量 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{V} = [V(s_1), V(s_2), ..., V(s_n)]^\top</span><script type="math/tex">\mathcal{V} = [V(s_1), V(s_2), ..., V(s_n)]^\top</script></span>，把奖励函数也表示成一个列向量 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{R} = [r(s_1), r(s_2), ..., r(s_n)]^\top</span><script type="math/tex">\mathcal{R} = [r(s_1), r(s_2), ..., r(s_n)]^\top</script></span>，然后就有贝尔曼方程的矩阵形式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\mathcal{V} = \mathcal{R} + \gamma \mathcal{P} \mathcal{V}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\mathcal{V} = \mathcal{R} + \gamma \mathcal{P} \mathcal{V}\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}
\begin{bmatrix}
   V(s_1) \\
   V(s_2) \\
   \vdots \\
   V(s_n)
\end{bmatrix} = \begin{bmatrix}
   r(s_1) \\
   r(s_2) \\
   \vdots \\
   r(s_n)
\end{bmatrix} + \gamma \begin{bmatrix}
   P(s_1|s_1) &amp; P(s_2|s_1) &amp; \cdots &amp; P(s_n|s_1) \\
   P(s_1|s_2) &amp; P(s_2|s_2) &amp; \cdots &amp; P(s_n|s_2) \\
   \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
   P(s_1|s_n) &amp; P(s_2|s_n) &amp; \cdots &amp; P(s_n|s_n)
\end{bmatrix} \begin{bmatrix}
   V(s_1) \\
   V(s_2) \\
   \vdots \\
   V(s_n)
\end{bmatrix}
\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}
\begin{bmatrix}
   V(s_1) \\
   V(s_2) \\
   \vdots \\
   V(s_n)
\end{bmatrix} = \begin{bmatrix}
   r(s_1) \\
   r(s_2) \\
   \vdots \\
   r(s_n)
\end{bmatrix} + \gamma \begin{bmatrix}
   P(s_1|s_1) & P(s_2|s_1) & \cdots & P(s_n|s_1) \\
   P(s_1|s_2) & P(s_2|s_2) & \cdots & P(s_n|s_2) \\
   \vdots & \vdots & \ddots & \vdots \\
   P(s_1|s_n) & P(s_2|s_n) & \cdots & P(s_n|s_n)
\end{bmatrix} \begin{bmatrix}
   V(s_1) \\
   V(s_2) \\
   \vdots \\
   V(s_n)
\end{bmatrix}
\end{equation}</script>
</div>
<p>在公式（7）中 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{R}</span><script type="math/tex">\mathcal{R}</script></span>、<span class="arithmatex"><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span>、<span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 都是已知量，只有 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{V}</span><script type="math/tex">\mathcal{V}</script></span> 是未知的，所以这里是可以直接求解其解析解的，做一下简单的变形即可，如下所示：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}\begin{split}
\mathcal{V} &amp;= \mathcal{R} + \gamma \mathcal{P} \mathcal{V} \\
(I - \gamma \mathcal{P}) \mathcal{V} &amp;= \mathcal{R} \\
\mathcal{V} &amp;= (I - \gamma \mathcal{P})^{-1} \mathcal{R}
\end{split}\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}\begin{split}
\mathcal{V} &= \mathcal{R} + \gamma \mathcal{P} \mathcal{V} \\
(I - \gamma \mathcal{P}) \mathcal{V} &= \mathcal{R} \\
\mathcal{V} &= (I - \gamma \mathcal{P})^{-1} \mathcal{R}
\end{split}\end{equation}</script>
</div>
<p>根据上式就可以通过 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{R}</span><script type="math/tex">\mathcal{R}</script></span>、<span class="arithmatex"><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span>、<span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 求解出 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{V}</span><script type="math/tex">\mathcal{V}</script></span>，这种方法的时间复杂度为 <span class="arithmatex"><span class="MathJax_Preview">O(n^3)</span><script type="math/tex">O(n^3)</script></span>，其中 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 是状态的个数，因此只有当 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 比较小时才能够使用这种方法，否则的话就太慢了。</p>
<h2 id="4mdp">4、马尔可夫决策过程(MDP)<a class="headerlink" href="#4mdp" title="Permanent link">#</a></h2>
<h3 id="41">4.1 决策过程<a class="headerlink" href="#41" title="Permanent link">#</a></h3>
<p>马尔可夫决策过程（Markov decision process）一般使用 <span class="arithmatex"><span class="MathJax_Preview">&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \gamma&gt;</span><script type="math/tex"><\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \gamma></script></span> 来表示。其中 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span> 表示有限个状态的集合，<span class="arithmatex"><span class="MathJax_Preview">\mathcal{A}</span><script type="math/tex">\mathcal{A}</script></span> 是动作的集合，<span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 是衰减因子。</p>
<p><span class="arithmatex"><span class="MathJax_Preview">r(s, a)</span><script type="math/tex">r(s, a)</script></span> 是奖励函数，注意此时的奖励函数既跟 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 有关，也跟 <span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 有关。</p>
<p><span class="arithmatex"><span class="MathJax_Preview">P(s^\prime|s, a)</span><script type="math/tex">P(s^\prime|s, a)</script></span> 是状态转移函数，其含义是在状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 执行动作 <span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 之后达到状态 <span class="arithmatex"><span class="MathJax_Preview">s^\prime</span><script type="math/tex">s^\prime</script></span> 的概率。在马尔可夫决策过程这里已经不再使用状态转移矩阵了，而是使用状态转移函数，这个更具有通用性。</p>
<h3 id="42">4.2 策略<a class="headerlink" href="#42" title="Permanent link">#</a></h3>
<p>智能体的策略（policy）使用符号 <span class="arithmatex"><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 来表示，即：<span class="arithmatex"><span class="MathJax_Preview">\pi(a|s) = P(A_t=a|S_t=s)</span><script type="math/tex">\pi(a|s) = P(A_t=a|S_t=s)</script></span>，其含义是在状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 下采取动作 <span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 的概率。</p>
<p>确定性策略（deterministic policy）指每个状态确定对应一个动作。</p>
<p>随机性策略（stochastic policy）指在每个状态时，采取哪个action是一个概率分布。在该概率分布中进行采样之后会得到一个具体的状态。</p>
<h3 id="43">4.3 状态价值函数<a class="headerlink" href="#43" title="Permanent link">#</a></h3>
<p>对于马尔可夫决策过程，其奖励函数为 <span class="arithmatex"><span class="MathJax_Preview">r(s, a)</span><script type="math/tex">r(s, a)</script></span>，可以看出其既与 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 有关，也与 <span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 有关，所以其价值函数也是有两个，分别是这一小节的状态价值函数和下一小节的动作价值函数。</p>
<p>状态价值函数（state-value function）使用符号 <span class="arithmatex"><span class="MathJax_Preview">V^\pi(s)</span><script type="math/tex">V^\pi(s)</script></span> 表示，定义是从状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 出发遵循策略 <span class="arithmatex"><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 所能获得的回报的期望。公式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}V^\pi(s) = E_\pi[G_t|S_t=s]\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}V^\pi(s) = E_\pi[G_t|S_t=s]\end{equation}</script>
</div>
<h3 id="44">4.4 动作价值函数<a class="headerlink" href="#44" title="Permanent link">#</a></h3>
<p>动作价值函数（action-value function）使用符号 <span class="arithmatex"><span class="MathJax_Preview">Q^\pi(s,a)</span><script type="math/tex">Q^\pi(s,a)</script></span> 表示，定义是在遵循策略 <span class="arithmatex"><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 时，对当前状态 <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 执行动作 <span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 之后，所能获得的回报的期望。公式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}Q^\pi(s, a)=E_\pi[G_t|S_t=s, A_t=a]\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}Q^\pi(s, a)=E_\pi[G_t|S_t=s, A_t=a]\end{equation}</script>
</div>
<p>然后看下状态价值函数 <span class="arithmatex"><span class="MathJax_Preview">V^\pi(s)</span><script type="math/tex">V^\pi(s)</script></span> 与动作价值函数 <span class="arithmatex"><span class="MathJax_Preview">Q^\pi(s, a)</span><script type="math/tex">Q^\pi(s, a)</script></span> 之间的转换关系公式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}V^\pi(s)=\sum_{a \in \mathcal{A}} \pi(a|s)Q^\pi(s, a)\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}V^\pi(s)=\sum_{a \in \mathcal{A}} \pi(a|s)Q^\pi(s, a)\end{equation}</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}Q^\pi(s, a)=r(s,a) + \sum_{s^\prime \in \mathcal{S}}P(s^\prime|s,a)V^\pi(s^\prime)\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}Q^\pi(s, a)=r(s,a) + \sum_{s^\prime \in \mathcal{S}}P(s^\prime|s,a)V^\pi(s^\prime)\end{equation}</script>
</div>
<h3 id="45">4.5 贝尔曼期望函数<a class="headerlink" href="#45" title="Permanent link">#</a></h3>
<p>还是要记着贝尔曼期望函数就是价值函数的递归式。那么，作为递归式，状态价值函数的贝尔曼期望函数就是 <span class="arithmatex"><span class="MathJax_Preview">V^\pi(s)</span><script type="math/tex">V^\pi(s)</script></span> 与 <span class="arithmatex"><span class="MathJax_Preview">V^\pi(s^\prime)</span><script type="math/tex">V^\pi(s^\prime)</script></span> 的关系。动作价值函数的贝尔曼期望函数就是 <span class="arithmatex"><span class="MathJax_Preview">Q^\pi(s,a)</span><script type="math/tex">Q^\pi(s,a)</script></span> 与 <span class="arithmatex"><span class="MathJax_Preview">Q^\pi(s^\prime, a^\prime)</span><script type="math/tex">Q^\pi(s^\prime, a^\prime)</script></span> 的关系。</p>
<p>状态价值函数的贝尔曼期望函数公式如下：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}V^\pi(s)=\sum_{a\in \mathcal{A}} \pi(a|s) \Big[r(s,a) + \gamma \sum_{s^\prime \in \mathcal{S}} P(s^\prime|s,a)V^\pi(s^\prime) \Big]\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}V^\pi(s)=\sum_{a\in \mathcal{A}} \pi(a|s) \Big[r(s,a) + \gamma \sum_{s^\prime \in \mathcal{S}} P(s^\prime|s,a)V^\pi(s^\prime) \Big]\end{equation}</script>
</div>
<p>动作价值函数的贝尔曼期望函数公式如下：</p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{equation}Q^\pi(s,a) = r(s,a) + \gamma \Big[ \sum_{s^\prime \in \mathcal{S}} P(s^\prime|s,a) \sum_{a^\prime \in \mathcal{A}} \pi(a^\prime|s^\prime) Q^\pi(s^\prime, a^\prime) \Big]\end{equation}</div>
<script type="math/tex; mode=display">\begin{equation}Q^\pi(s,a) = r(s,a) + \gamma \Big[ \sum_{s^\prime \in \mathcal{S}} P(s^\prime|s,a) \sum_{a^\prime \in \mathcal{A}} \pi(a^\prime|s^\prime) Q^\pi(s^\prime, a^\prime) \Big]\end{equation}</script>
</div>
<p>关于这两个贝尔曼期望方程的直观理解，和第 3.4 部分对贝尔曼方程的直观说明是基本相似的。</p>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">#</a></h2>
<ul>
<li><a href="https://hrl.boyuai.com/chapter/1/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B">https://hrl.boyuai.com/chapter/1/马尔可夫决策过程</a></li>
</ul></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2021 Microsoft Research;<a href="https://beian.miit.gov.cn/">备案号：京ICP备2022025323号-1</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="../../../mathjax-config.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
